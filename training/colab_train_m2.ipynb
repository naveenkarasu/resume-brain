{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2 Resume Extractor Training (Colab + T4 GPU)\n",
    "\n",
    "Fine-tunes **yashpwr/resume-ner-bert-v2** for BIO NER on resumes with 14 entity types (29 labels).\n",
    "Two-phase training: frozen layers 0-8 for 2 epochs, then all layers unfrozen for 6 epochs.\n",
    "\n",
    "## Setup\n",
    "1. Upload `m2_training_data.zip` (~276MB) to your Google Drive\n",
    "2. Run all cells\n",
    "3. Download the trained model from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q transformers datasets seqeval accelerate pandas pyarrow pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Mount Google Drive and extract training data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# Look for the zip in common Drive locations\n",
    "zip_candidates = [\n",
    "    '/content/drive/MyDrive/m2_training_data.zip',\n",
    "    '/content/drive/MyDrive/Colab/m2_training_data.zip',\n",
    "    '/content/drive/MyDrive/training/m2_training_data.zip',\n",
    "]\n",
    "zip_path = None\n",
    "for p in zip_candidates:\n",
    "    if os.path.exists(p):\n",
    "        zip_path = p\n",
    "        break\n",
    "\n",
    "if zip_path is None:\n",
    "    # Fallback: search Drive root for the file\n",
    "    for root, dirs, files in os.walk('/content/drive/MyDrive/'):\n",
    "        if 'm2_training_data.zip' in files:\n",
    "            zip_path = os.path.join(root, 'm2_training_data.zip')\n",
    "            break\n",
    "        # Don't recurse too deep\n",
    "        if root.count(os.sep) > 5:\n",
    "            break\n",
    "\n",
    "if zip_path:\n",
    "    print(f\"Found zip at: {zip_path}\")\n",
    "    !unzip -o \"{zip_path}\" -d /content/m2_data\n",
    "else:\n",
    "    print(\"ERROR: m2_training_data.zip not found in Google Drive!\")\n",
    "    print(\"Please upload m2_training_data.zip to your Google Drive root folder.\")\n",
    "    print(\"Then re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data preparation - load and unify all 5 resume NER datasets\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DATA_DIR = Path(\"/content/m2_data\")\n",
    "\n",
    "# --- Entity types and BIO labels ---\n",
    "ENTITY_TYPES = [\n",
    "    \"NAME\", \"EMAIL\", \"PHONE\", \"LOCATION\", \"DESIGNATION\", \"COMPANY\",\n",
    "    \"DEGREE\", \"GRADUATION_YEAR\", \"COLLEGE_NAME\", \"YEARS_OF_EXPERIENCE\",\n",
    "    \"SKILLS\", \"CERTIFICATION\", \"PROJECT_NAME\", \"PROJECT_TECHNOLOGY\",\n",
    "]\n",
    "\n",
    "LABELS = [\"O\"]\n",
    "for etype in ENTITY_TYPES:\n",
    "    LABELS.append(f\"B-{etype}\")\n",
    "    LABELS.append(f\"I-{etype}\")\n",
    "\n",
    "LABEL2ID = {label: idx for idx, label in enumerate(LABELS)}\n",
    "ID2LABEL = {idx: label for idx, label in enumerate(LABELS)}\n",
    "\n",
    "print(f\"Entity types: {len(ENTITY_TYPES)}\")\n",
    "print(f\"Total labels (BIO): {len(LABELS)}\")\n",
    "\n",
    "# --- Label normalization map ---\n",
    "_LABEL_NORMALIZE = {\n",
    "    \"Name\": \"NAME\", \"name\": \"NAME\",\n",
    "    \"EMAIL\": \"EMAIL\", \"Email Address\": \"EMAIL\", \"email\": \"EMAIL\",\n",
    "    \"Phone\": \"PHONE\", \"phone\": \"PHONE\", \"PHONE\": \"PHONE\",\n",
    "    \"Location\": \"LOCATION\", \"location\": \"LOCATION\", \"LOCATION\": \"LOCATION\",\n",
    "    \"Designation\": \"DESIGNATION\", \"designation\": \"DESIGNATION\", \"DESIGNATION\": \"DESIGNATION\",\n",
    "    \"Companies worked at\": \"COMPANY\", \"Company\": \"COMPANY\", \"company\": \"COMPANY\", \"COMPANY\": \"COMPANY\",\n",
    "    \"Degree\": \"DEGREE\", \"degree\": \"DEGREE\", \"DEGREE\": \"DEGREE\",\n",
    "    \"Graduation Year\": \"GRADUATION_YEAR\", \"graduation_year\": \"GRADUATION_YEAR\",\n",
    "    \"College Name\": \"COLLEGE_NAME\", \"college_name\": \"COLLEGE_NAME\", \"COLLEGE\": \"COLLEGE_NAME\",\n",
    "    \"Years of Experience\": \"YEARS_OF_EXPERIENCE\", \"years_of_experience\": \"YEARS_OF_EXPERIENCE\", \"Experience\": \"YEARS_OF_EXPERIENCE\",\n",
    "    \"Skills\": \"SKILLS\", \"skills\": \"SKILLS\", \"SKILLS\": \"SKILLS\",\n",
    "    \"Certification\": \"CERTIFICATION\", \"certification\": \"CERTIFICATION\", \"CERTIFICATION\": \"CERTIFICATION\",\n",
    "    \"Project\": \"PROJECT_NAME\", \"project\": \"PROJECT_NAME\", \"PROJECT\": \"PROJECT_NAME\",\n",
    "    \"Technology\": \"PROJECT_TECHNOLOGY\", \"technology\": \"PROJECT_TECHNOLOGY\",\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_label(raw_label):\n",
    "    return _LABEL_NORMALIZE.get(raw_label)\n",
    "\n",
    "\n",
    "def _tokenize_with_offsets(text):\n",
    "    tokens, offsets = [], []\n",
    "    for m in re.finditer(r\"\\S+\", text):\n",
    "        tokens.append(m.group())\n",
    "        offsets.append((m.start(), m.end()))\n",
    "    return tokens, offsets\n",
    "\n",
    "\n",
    "def _bio_tags_from_char_spans(tokens, char_offsets, spans):\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "    for span in spans:\n",
    "        s_start, s_end, label = span[\"start\"], span[\"end\"], span[\"label\"]\n",
    "        first = True\n",
    "        for idx, (t_start, t_end) in enumerate(char_offsets):\n",
    "            if t_end <= s_start or t_start >= s_end:\n",
    "                continue\n",
    "            prefix = \"B\" if first else \"I\"\n",
    "            tag = f\"{prefix}-{label}\"\n",
    "            if tag in LABEL2ID:\n",
    "                tags[idx] = tag\n",
    "                first = False\n",
    "    return tags\n",
    "\n",
    "\n",
    "def _label_to_id(tags):\n",
    "    return [LABEL2ID.get(t, 0) for t in tags]\n",
    "\n",
    "\n",
    "# --- Dataset Loaders ---\n",
    "\n",
    "def load_yashpwr():\n",
    "    parquet_path = DATA_DIR / \"yashpwr_resume_ner\" / \"train.parquet\"\n",
    "    if not parquet_path.exists():\n",
    "        logger.warning(\"yashpwr not found -- skipping.\")\n",
    "        return []\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    if \"tokens\" not in df.columns or \"ner_tags\" not in df.columns:\n",
    "        return []\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        tokens, raw_tags = row[\"tokens\"], row[\"ner_tags\"]\n",
    "        if not isinstance(tokens, list) or not isinstance(raw_tags, list):\n",
    "            continue\n",
    "        if len(tokens) != len(raw_tags):\n",
    "            continue\n",
    "        records.append({\"tokens\": tokens, \"ner_tags\": [int(t) for t in raw_tags], \"source\": \"yashpwr\"})\n",
    "    logger.info(\"yashpwr: %d sequences\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "def load_dataturks():\n",
    "    json_path = DATA_DIR / \"dataturks_resume_ner\" / \"Entity Recognition in Resumes.json\"\n",
    "    if not json_path.exists():\n",
    "        logger.warning(\"DataTurks not found -- skipping.\")\n",
    "        return []\n",
    "    records = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            content = entry.get(\"content\", \"\")\n",
    "            annotations = entry.get(\"annotation\", [])\n",
    "            if not content or not annotations:\n",
    "                continue\n",
    "            tokens, offsets = _tokenize_with_offsets(content)\n",
    "            if len(tokens) < 3:\n",
    "                continue\n",
    "            spans = []\n",
    "            for ann in annotations:\n",
    "                raw_labels = ann.get(\"label\", [])\n",
    "                if isinstance(raw_labels, str):\n",
    "                    raw_labels = [raw_labels]\n",
    "                points = ann.get(\"points\", [])\n",
    "                if not points:\n",
    "                    continue\n",
    "                for raw_label in raw_labels:\n",
    "                    norm = _normalize_label(raw_label)\n",
    "                    if norm is None:\n",
    "                        continue\n",
    "                    for pt in points:\n",
    "                        start, end = pt.get(\"start\"), pt.get(\"end\")\n",
    "                        if start is None or end is None:\n",
    "                            continue\n",
    "                        spans.append({\"start\": start, \"end\": end + 1, \"label\": norm})\n",
    "            tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n",
    "            for i in range(0, len(tokens), 128):\n",
    "                ct, ctags = tokens[i:i+128], tags[i:i+128]\n",
    "                if len(ct) >= 3:\n",
    "                    records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"dataturks\"})\n",
    "    logger.info(\"DataTurks: %d sequences\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "def load_mehyaar():\n",
    "    base_dir = DATA_DIR / \"mehyaar_ner_cvs\" / \"ResumesJsonAnnotated\" / \"ResumesJsonAnnotated\"\n",
    "    if not base_dir.exists():\n",
    "        logger.warning(\"Mehyaar not found -- skipping.\")\n",
    "        return []\n",
    "    records = []\n",
    "    for jf in sorted(base_dir.glob(\"*.json\")):\n",
    "        try:\n",
    "            with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except (json.JSONDecodeError, UnicodeDecodeError):\n",
    "            continue\n",
    "        text = data.get(\"text\", data.get(\"content\", \"\"))\n",
    "        annotations = data.get(\"annotations\", data.get(\"annotation\", []))\n",
    "        if not text or not annotations:\n",
    "            continue\n",
    "        tokens, offsets = _tokenize_with_offsets(text)\n",
    "        if len(tokens) < 3:\n",
    "            continue\n",
    "        spans = []\n",
    "        for ann in annotations:\n",
    "            if isinstance(ann, dict):\n",
    "                label = ann.get(\"label\", ann.get(\"type\", \"\"))\n",
    "                start = ann.get(\"start\", ann.get(\"startOffset\"))\n",
    "                end = ann.get(\"end\", ann.get(\"endOffset\"))\n",
    "            elif isinstance(ann, (list, tuple)) and len(ann) >= 3:\n",
    "                start, end, label = ann[0], ann[1], ann[2]\n",
    "            else:\n",
    "                continue\n",
    "            if start is None or end is None or not label:\n",
    "                continue\n",
    "            norm = _normalize_label(label)\n",
    "            if norm is None:\n",
    "                if label.upper() in [e.upper() for e in ENTITY_TYPES]:\n",
    "                    norm = label.upper()\n",
    "                else:\n",
    "                    continue\n",
    "            spans.append({\"start\": int(start), \"end\": int(end), \"label\": norm})\n",
    "        tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n",
    "        for i in range(0, len(tokens), 128):\n",
    "            ct, ctags = tokens[i:i+128], tags[i:i+128]\n",
    "            if len(ct) >= 3:\n",
    "                records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"mehyaar\"})\n",
    "    logger.info(\"Mehyaar: %d sequences\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "def load_datasetmaster():\n",
    "    parquet_path = DATA_DIR / \"datasetmaster_resumes\" / \"train.parquet\"\n",
    "    if not parquet_path.exists():\n",
    "        logger.warning(\"DatasetMaster not found -- skipping.\")\n",
    "        return []\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    records = []\n",
    "    text_col = None\n",
    "    for candidate in [\"resume_text\", \"text\", \"content\", \"resume\", \"Resume\"]:\n",
    "        if candidate in df.columns:\n",
    "            text_col = candidate\n",
    "            break\n",
    "    if text_col is None:\n",
    "        # Synthesize from structured fields\n",
    "        for _, row in df.iterrows():\n",
    "            tokens_all, tags_all = [], []\n",
    "            for col, label in [(\"skills\", \"SKILLS\"), (\"education\", \"DEGREE\"), (\"projects\", \"PROJECT_NAME\")]:\n",
    "                val = row.get(col)\n",
    "                if isinstance(val, str) and val.strip():\n",
    "                    toks, _ = _tokenize_with_offsets(val)\n",
    "                    tokens_all.extend(toks)\n",
    "                    tags_all.extend([f\"B-{label}\"] + [f\"I-{label}\"] * (len(toks) - 1))\n",
    "                elif isinstance(val, list):\n",
    "                    for item in val:\n",
    "                        if isinstance(item, str) and item.strip():\n",
    "                            toks, _ = _tokenize_with_offsets(item)\n",
    "                            tokens_all.extend(toks)\n",
    "                            tags_all.extend([f\"B-{label}\"] + [f\"I-{label}\"] * (len(toks) - 1))\n",
    "            if len(tokens_all) >= 5:\n",
    "                records.append({\"tokens\": tokens_all, \"ner_tags\": _label_to_id(tags_all), \"source\": \"datasetmaster\"})\n",
    "        logger.info(\"DatasetMaster (structured): %d sequences\", len(records))\n",
    "        return records\n",
    "    # Weak supervision from structured columns onto text\n",
    "    field_label_map = {\n",
    "        \"skills\": \"SKILLS\", \"education\": \"DEGREE\", \"company\": \"COMPANY\",\n",
    "        \"designation\": \"DESIGNATION\", \"college\": \"COLLEGE_NAME\", \"degree\": \"DEGREE\",\n",
    "        \"projects\": \"PROJECT_NAME\", \"certification\": \"CERTIFICATION\", \"certifications\": \"CERTIFICATION\",\n",
    "    }\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[text_col]\n",
    "        if not isinstance(text, str) or len(text) < 30:\n",
    "            continue\n",
    "        tokens, offsets = _tokenize_with_offsets(text[:2000])\n",
    "        if len(tokens) < 5:\n",
    "            continue\n",
    "        spans = []\n",
    "        for col, label in field_label_map.items():\n",
    "            val = row.get(col)\n",
    "            if val is None:\n",
    "                continue\n",
    "            search_terms = []\n",
    "            if isinstance(val, str) and val.strip():\n",
    "                search_terms = [val.strip()]\n",
    "            elif isinstance(val, list):\n",
    "                search_terms = [str(v).strip() for v in val if isinstance(v, str) and v.strip()]\n",
    "            for term in search_terms[:10]:\n",
    "                try:\n",
    "                    for m in re.finditer(re.escape(term[:100]), text[:2000], re.IGNORECASE):\n",
    "                        spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": label})\n",
    "                        break\n",
    "                except re.error:\n",
    "                    continue\n",
    "        tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n",
    "        for i in range(0, len(tokens), 128):\n",
    "            ct, ctags = tokens[i:i+128], tags[i:i+128]\n",
    "            if len(ct) >= 3:\n",
    "                records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"datasetmaster\"})\n",
    "    logger.info(\"DatasetMaster: %d sequences\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "def load_djinni():\n",
    "    parquet_path = DATA_DIR / \"djinni_candidates\" / \"train.parquet\"\n",
    "    if not parquet_path.exists():\n",
    "        logger.warning(\"Djinni not found -- skipping.\")\n",
    "        return []\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    if len(df) > 15000:\n",
    "        df = df.sample(n=15000, random_state=42)\n",
    "    records = []\n",
    "    text_col = None\n",
    "    for candidate in [\"description\", \"text\", \"bio\", \"summary\", \"content\", \"experience\"]:\n",
    "        if candidate in df.columns:\n",
    "            text_col = candidate\n",
    "            break\n",
    "    skill_col = None\n",
    "    for candidate in [\"skills\", \"keywords\", \"technologies\"]:\n",
    "        if candidate in df.columns:\n",
    "            skill_col = candidate\n",
    "            break\n",
    "    position_col = None\n",
    "    for candidate in [\"position\", \"title\", \"designation\", \"job_title\"]:\n",
    "        if candidate in df.columns:\n",
    "            position_col = candidate\n",
    "            break\n",
    "    if text_col is None:\n",
    "        logger.warning(\"Djinni: no text column found. Columns: %s\", df.columns.tolist())\n",
    "        return []\n",
    "    exp_pattern = re.compile(r\"\\b(\\d+)\\+?\\s*years?\\b\", re.IGNORECASE)\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.get(text_col, \"\")\n",
    "        if not isinstance(text, str) or len(text) < 20:\n",
    "            continue\n",
    "        text = text[:1500]\n",
    "        tokens, offsets = _tokenize_with_offsets(text)\n",
    "        if len(tokens) < 5:\n",
    "            continue\n",
    "        spans = []\n",
    "        for m in exp_pattern.finditer(text):\n",
    "            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"YEARS_OF_EXPERIENCE\"})\n",
    "        if skill_col:\n",
    "            skills_val = row.get(skill_col, \"\")\n",
    "            if isinstance(skills_val, str):\n",
    "                skill_list = [s.strip() for s in skills_val.split(\",\") if s.strip()]\n",
    "            elif isinstance(skills_val, list):\n",
    "                skill_list = [str(s).strip() for s in skills_val if s]\n",
    "            else:\n",
    "                skill_list = []\n",
    "            for skill in skill_list[:15]:\n",
    "                try:\n",
    "                    for m in re.finditer(re.escape(skill), text, re.IGNORECASE):\n",
    "                        spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"SKILLS\"})\n",
    "                        break\n",
    "                except re.error:\n",
    "                    continue\n",
    "        if position_col:\n",
    "            pos_val = row.get(position_col, \"\")\n",
    "            if isinstance(pos_val, str) and pos_val.strip():\n",
    "                try:\n",
    "                    for m in re.finditer(re.escape(pos_val.strip()), text, re.IGNORECASE):\n",
    "                        spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"DESIGNATION\"})\n",
    "                        break\n",
    "                except re.error:\n",
    "                    pass\n",
    "        tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n",
    "        if any(t != \"O\" for t in tags):\n",
    "            for i in range(0, len(tokens), 128):\n",
    "                ct, ctags = tokens[i:i+128], tags[i:i+128]\n",
    "                if len(ct) >= 3:\n",
    "                    records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"djinni\"})\n",
    "    logger.info(\"Djinni: %d sequences\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "# --- Load all datasets ---\n",
    "print(\"Loading all 5 datasets...\")\n",
    "all_records = []\n",
    "for name, loader in [(\"yashpwr\", load_yashpwr), (\"DataTurks\", load_dataturks),\n",
    "                      (\"Mehyaar\", load_mehyaar), (\"DatasetMaster\", load_datasetmaster),\n",
    "                      (\"Djinni\", load_djinni)]:\n",
    "    try:\n",
    "        recs = loader()\n",
    "        all_records.extend(recs)\n",
    "        print(f\"  {name}: {len(recs)} sequences (total: {len(all_records)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {name}: FAILED - {e}\")\n",
    "\n",
    "# Remove internal flags\n",
    "for rec in all_records:\n",
    "    rec.pop(\"_raw_tags\", None)\n",
    "\n",
    "print(f\"\\nTotal unified: {len(all_records)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4.5: yashpwr Weak Supervision NER\n\n**Problem:** The `yashpwr/resume-ner-training-data` dataset was expected to have BIO-tagged NER\ncolumns (`tokens`, `ner_tags`), but actually contains **chat/instruction data** with a `messages`\ncolumn in system/user/assistant format:\n- **system**: \"You are an expert resume assistant...\"\n- **user**: \"Please summarize the following resume:\\n\\n`[FULL RESUME TEXT]`\"\n- **assistant**: \"`[SUMMARY]`\"\n\n**Solution:** We extract the full resume text from the user message and apply **weak supervision**\n(regex + section parsing) to automatically generate BIO tags. This is noisier than gold-standard\nannotations but adds ~20K+ sequences covering entity types that other datasets may lack.\n\n**How it works:**\n1. Extract resume text from the `user` message (everything after the instruction prompt)\n2. Detect section headers: `Skills`, `Education`, `Professional Experience`, etc.\n3. Use regex patterns to tag entities within each section:\n   - **SKILLS**: Comma/newline-separated items in Skills sections\n   - **DESIGNATION**: Job titles at the start of experience blocks (e.g. \"General Manager\")\n   - **COMPANY**: Text following \"Company Name\" patterns\n   - **LOCATION**: \"City , State\" patterns\n   - **DEGREE**: Education qualifiers like \"BS\", \"MBA\", \"Master\"\n   - **COLLEGE_NAME**: Institution names in Education sections\n   - **GRADUATION_YEAR**: 4-digit years in Education sections\n   - **YEARS_OF_EXPERIENCE**: \"X+ years\" patterns anywhere\n   - **EMAIL**: Standard email regex\n   - **PHONE**: Phone number patterns\n\n**Label quality:** Weak supervision = ~70-80% accuracy. Mixing it with gold-standard data\n(Mehyaar, DataTurks) still improves model coverage for underrepresented entity types.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 4.5: yashpwr Weak Supervision - extract NER from chat/instruction format\n#\n# The yashpwr dataset has 22,855 resumes in chat format (messages column).\n# We extract resume text from the \"user\" message and apply regex-based\n# weak supervision to generate BIO tags automatically.\n\ndef load_yashpwr_weak_supervision():\n    \"\"\"Parse yashpwr chat messages into weakly-labeled NER sequences.\"\"\"\n    parquet_path = DATA_DIR / \"yashpwr_resume_ner\" / \"train.parquet\"\n    if not parquet_path.exists():\n        print(\"yashpwr parquet not found -- skipping weak supervision.\")\n        return []\n\n    df = pd.read_parquet(parquet_path)\n    if \"messages\" not in df.columns:\n        print(f\"yashpwr has no 'messages' column (cols: {df.columns.tolist()}) -- skipping.\")\n        return []\n\n    print(f\"yashpwr: {len(df)} rows with chat messages. Applying weak supervision...\")\n\n    # --- Regex patterns for entity detection ---\n\n    # Section headers that help us know what context we're in\n    SECTION_PATTERNS = {\n        \"skills\": re.compile(\n            r\"^(Skills|Technical Skills|Core Competencies|Areas of Expertise|\"\n            r\"Skill Highlights|Additional Skills|Computer Skills|Software Skills)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n        \"education\": re.compile(\n            r\"^(Education|Academic Background|Educational Background|Qualifications)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n        \"experience\": re.compile(\n            r\"^(Professional Experience|Work Experience|Experience|Employment|\"\n            r\"Employment History|Work History|Professional Background|Career History)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n        \"certifications\": re.compile(\n            r\"^(Certifications?|Licenses?|Professional Certifications?|\"\n            r\"Licenses? and Certifications?)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n    }\n\n    # Entity-level patterns\n    EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\")\n    PHONE_RE = re.compile(r\"(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\")\n    YEARS_EXP_RE = re.compile(r\"\\b(\\d{1,2})\\+?\\s*(?:years?|yrs?)\\s*(?:of\\s+)?(?:experience)?\\b\", re.IGNORECASE)\n    YEAR_RE = re.compile(r\"\\b(19[89]\\d|20[0-2]\\d)\\b\")\n    LOCATION_RE = re.compile(r\"\\b([A-Z][a-z]+(?:\\s[A-Z][a-z]+)?)\\s*,\\s*([A-Z]{2})\\b\")\n    COMPANY_RE = re.compile(r\"Company\\s+Name\\s+(.+?)(?:\\s+City\\s*,|\\s*$)\", re.IGNORECASE | re.MULTILINE)\n\n    # Degree patterns\n    DEGREE_RE = re.compile(\n        r\"\\b(Ph\\.?D\\.?|M\\.?D\\.?|J\\.?D\\.?|M\\.?B\\.?A\\.?|M\\.?S\\.?|B\\.?S\\.?|B\\.?A\\.?|M\\.?A\\.?|\"\n        r\"Bachelor(?:'?s)?(?:\\s+of\\s+\\w+)?|Master(?:'?s)?(?:\\s+of\\s+\\w+)?|\"\n        r\"Associate(?:'?s)?(?:\\s+of\\s+\\w+)?|Doctorate|Doctor of)\\b\",\n        re.IGNORECASE\n    )\n\n    # Common job title patterns (for DESIGNATION)\n    TITLE_RE = re.compile(\n        r\"\\b((?:Senior|Junior|Lead|Chief|Head|Principal|Staff|Associate|Assistant|Executive|\"\n        r\"Vice President|VP|Director|Manager|Coordinator|Specialist|Analyst|Engineer|\"\n        r\"Developer|Designer|Consultant|Administrator|Supervisor|Officer|Architect|\"\n        r\"Technician|Representative|Advisor|Strategist|Planner)\"\n        r\"(?:\\s+(?:of|for))?\"\n        r\"(?:\\s+\\w+){0,3})\"\n        r\"(?=\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December|\\d{4}|Company|$))\",\n        re.IGNORECASE | re.MULTILINE\n    )\n\n    records = []\n    skipped = 0\n\n    for idx, row in df.iterrows():\n        messages = row[\"messages\"]\n        if not isinstance(messages, (list, np.ndarray)):\n            skipped += 1\n            continue\n\n        # Extract resume text from user message\n        resume_text = \"\"\n        for msg in messages:\n            if isinstance(msg, dict) and msg.get(\"role\") == \"user\":\n                content = msg.get(\"content\", \"\")\n                # Remove the instruction prompt, keep just the resume\n                for separator in [\n                    \"following resume:\\n\\n\",\n                    \"following resume:\\n\",\n                    \"this resume:\\n\\n\",\n                    \"this resume:\\n\",\n                    \"resume:\\n\\n\",\n                    \"resume:\\n\",\n                ]:\n                    if separator in content:\n                        resume_text = content.split(separator, 1)[1]\n                        break\n                if not resume_text:\n                    # If no separator found, use the whole content if it's long enough\n                    if len(content) > 200:\n                        resume_text = content\n                break\n\n        if len(resume_text) < 50:\n            skipped += 1\n            continue\n\n        # Truncate very long resumes\n        resume_text = resume_text[:3000]\n        tokens, offsets = _tokenize_with_offsets(resume_text)\n        if len(tokens) < 10:\n            skipped += 1\n            continue\n\n        spans = []\n\n        # --- 1. Email ---\n        for m in EMAIL_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"EMAIL\"})\n\n        # --- 2. Phone ---\n        for m in PHONE_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"PHONE\"})\n\n        # --- 3. Years of experience ---\n        for m in YEARS_EXP_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"YEARS_OF_EXPERIENCE\"})\n\n        # --- 4. Location (City, ST) ---\n        for m in LOCATION_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"LOCATION\"})\n\n        # --- 5. Company Name patterns ---\n        for m in COMPANY_RE.finditer(resume_text):\n            company = m.group(1).strip()\n            if len(company) > 2:\n                spans.append({\"start\": m.start(1), \"end\": m.start(1) + len(company), \"label\": \"COMPANY\"})\n\n        # --- 6. Degrees ---\n        for m in DEGREE_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"DEGREE\"})\n\n        # --- 7. Section-aware tagging ---\n        # Find section boundaries\n        sections = []\n        for sec_name, sec_re in SECTION_PATTERNS.items():\n            for m in sec_re.finditer(resume_text):\n                sections.append((m.start(), sec_name))\n        sections.sort(key=lambda x: x[0])\n\n        # Skills section: tag comma/newline separated items\n        for i, (sec_start, sec_name) in enumerate(sections):\n            sec_end = sections[i + 1][0] if i + 1 < len(sections) else len(resume_text)\n            sec_text = resume_text[sec_start:sec_end]\n\n            if sec_name == \"skills\":\n                # Skip the header line, get content\n                lines = sec_text.split(\"\\n\")[1:]\n                content = \" \".join(lines).strip()\n                # Split by commas, semicolons, or bullet-like separators\n                skill_items = re.split(r\"[,;|]|\\band\\b\", content)\n                for item in skill_items:\n                    item = item.strip().strip(\".\")\n                    if 2 < len(item) < 50 and not item[0].isdigit():\n                        try:\n                            for m in re.finditer(re.escape(item), resume_text[sec_start:sec_end]):\n                                abs_start = sec_start + m.start()\n                                abs_end = sec_start + m.end()\n                                spans.append({\"start\": abs_start, \"end\": abs_end, \"label\": \"SKILLS\"})\n                                break\n                        except re.error:\n                            continue\n\n            elif sec_name == \"education\":\n                # Tag graduation years in education section\n                for m in YEAR_RE.finditer(sec_text):\n                    abs_start = sec_start + m.start()\n                    abs_end = sec_start + m.end()\n                    spans.append({\"start\": abs_start, \"end\": abs_end, \"label\": \"GRADUATION_YEAR\"})\n\n                # Tag college names: lines with known institution keywords\n                for m in re.finditer(\n                    r\"((?:University|College|Institute|School|Academy|\"\n                    r\"Polytechnic|Conservatory)(?:\\s+of)?\\s+[\\w\\s]+?)(?:\\s*[-,\\n]|$)\",\n                    sec_text, re.IGNORECASE\n                ):\n                    name = m.group(1).strip()\n                    if len(name) > 5:\n                        abs_start = sec_start + m.start(1)\n                        abs_end = sec_start + m.start(1) + len(name)\n                        spans.append({\"start\": abs_start, \"end\": abs_end, \"label\": \"COLLEGE_NAME\"})\n\n            elif sec_name == \"certifications\":\n                # Tag entire non-empty lines as CERTIFICATION\n                for line in sec_text.split(\"\\n\")[1:]:\n                    line = line.strip()\n                    if len(line) > 5 and not re.match(r\"^\\d{4}\", line):\n                        try:\n                            for m in re.finditer(re.escape(line[:80]), resume_text):\n                                spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"CERTIFICATION\"})\n                                break\n                        except re.error:\n                            continue\n\n        # --- 8. Job titles (DESIGNATION) near date patterns ---\n        for m in TITLE_RE.finditer(resume_text):\n            title = m.group(1).strip()\n            if len(title) > 3:\n                spans.append({\"start\": m.start(1), \"end\": m.start(1) + len(title), \"label\": \"DESIGNATION\"})\n\n        # Only keep records that have at least some entity tags\n        if not spans:\n            skipped += 1\n            continue\n\n        # Remove overlapping spans (keep longest)\n        spans.sort(key=lambda s: (s[\"start\"], -(s[\"end\"] - s[\"start\"])))\n        filtered_spans = []\n        last_end = -1\n        for s in spans:\n            if s[\"start\"] >= last_end:\n                filtered_spans.append(s)\n                last_end = s[\"end\"]\n\n        tags = _bio_tags_from_char_spans(tokens, offsets, filtered_spans)\n\n        # Chunk into 128-token sequences\n        for i in range(0, len(tokens), 128):\n            ct = tokens[i:i + 128]\n            ctags = tags[i:i + 128]\n            if len(ct) >= 5 and any(t != \"O\" for t in ctags):\n                records.append({\n                    \"tokens\": ct,\n                    \"ner_tags\": _label_to_id(ctags),\n                    \"source\": \"yashpwr_weak\",\n                })\n\n    # Summary stats\n    entity_counts = {}\n    for rec in records:\n        for tag_id in rec[\"ner_tags\"]:\n            label = ID2LABEL.get(tag_id, \"O\")\n            if label != \"O\" and label.startswith(\"B-\"):\n                etype = label[2:]\n                entity_counts[etype] = entity_counts.get(etype, 0) + 1\n\n    print(f\"\\nyashpwr weak supervision results:\")\n    print(f\"  Processed: {len(df) - skipped} resumes (skipped {skipped})\")\n    print(f\"  Generated: {len(records)} sequences\")\n    print(f\"  Entity counts (B- tags):\")\n    for etype, count in sorted(entity_counts.items(), key=lambda x: -x[1]):\n        print(f\"    {etype}: {count}\")\n\n    return records\n\n\n# --- Run it and add to all_records ---\nyashpwr_weak = load_yashpwr_weak_supervision()\nall_records.extend(yashpwr_weak)\nprint(f\"\\nTotal after yashpwr weak supervision: {len(all_records)} sequences\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Clean and split data\n",
    "\n",
    "# Clean records\n",
    "cleaned = []\n",
    "for rec in all_records:\n",
    "    tokens = [str(t) if not isinstance(t, str) else t for t in rec.get(\"tokens\", [])]\n",
    "    tags = [int(t) if not isinstance(t, int) else t for t in rec.get(\"ner_tags\", [])]\n",
    "    if not tokens or len(tokens) != len(tags):\n",
    "        continue\n",
    "    if any(t in (\"nan\", \"None\", \"\") for t in tokens):\n",
    "        continue\n",
    "    # Strip surrogate characters that break Arrow/UTF-8\n",
    "    tokens = [t.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\") for t in tokens]\n",
    "    source = str(rec.get(\"source\", \"unknown\")).encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")\n",
    "    cleaned.append({\"tokens\": tokens, \"ner_tags\": tags, \"source\": source})\n",
    "\n",
    "print(f\"Cleaned: {len(all_records)} -> {len(cleaned)} records (dropped {len(all_records) - len(cleaned)})\")\n",
    "\n",
    "# Split into train/val/test\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "rng = random.Random(42)\n",
    "rng.shuffle(cleaned)\n",
    "\n",
    "n = len(cleaned)\n",
    "n_train = int(n * TRAIN_RATIO)\n",
    "n_val = int(n * VAL_RATIO)\n",
    "\n",
    "splits = {\n",
    "    \"train\": cleaned[:n_train],\n",
    "    \"validation\": cleaned[n_train:n_train + n_val],\n",
    "    \"test\": cleaned[n_train + n_val:],\n",
    "}\n",
    "\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(Value(\"int32\")),\n",
    "    \"source\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "dd = DatasetDict()\n",
    "for split_name, split_records in splits.items():\n",
    "    dd[split_name] = Dataset.from_dict(\n",
    "        {\n",
    "            \"tokens\": [r[\"tokens\"] for r in split_records],\n",
    "            \"ner_tags\": [r[\"ner_tags\"] for r in split_records],\n",
    "            \"source\": [r[\"source\"] for r in split_records],\n",
    "        },\n",
    "        features=features,\n",
    "    )\n",
    "    print(f\"  {split_name}: {len(split_records)} examples\")\n",
    "\n",
    "train_ds, val_ds, test_ds = dd[\"train\"], dd[\"validation\"], dd[\"test\"]\n",
    "print(f\"\\nTrain: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Tokenize and align labels\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "BASE_MODEL = \"yashpwr/resume-ner-bert-v2\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def tokenize_and_align(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        aligned = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                aligned.append(labels[word_id])\n",
    "            else:\n",
    "                aligned.append(-100)\n",
    "            previous_word_id = word_id\n",
    "        all_labels.append(aligned)\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing train...\")\n",
    "train_tok = train_ds.map(tokenize_and_align, batched=True, remove_columns=train_ds.column_names)\n",
    "print(\"Tokenizing val...\")\n",
    "val_tok = val_ds.map(tokenize_and_align, batched=True, remove_columns=val_ds.column_names)\n",
    "print(\"Tokenizing test...\")\n",
    "test_tok = test_ds.map(tokenize_and_align, batched=True, remove_columns=test_ds.column_names)\n",
    "print(f\"Tokenized: train={len(train_tok)}, val={len(val_tok)}, test={len(test_tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model setup + two-phase training\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Label list for metrics (must match LABELS from cell 4)\n",
    "label_list = LABELS\n",
    "id2label = ID2LABEL\n",
    "label2id = LABEL2ID\n",
    "\n",
    "# Load model with resized classifier head\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,  # classifier head changes from 11 to 14 entity types\n",
    ")\n",
    "print(f\"Model loaded: {BASE_MODEL} with {len(label_list)} labels\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels, true_preds = [], []\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        t_labels, t_preds = [], []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            t_labels.append(label_list[l])\n",
    "            t_preds.append(label_list[p])\n",
    "        true_labels.append(t_labels)\n",
    "        true_preds.append(t_preds)\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }\n",
    "\n",
    "\n",
    "def freeze_bert_layers(model, layer_indices):\n",
    "    for idx in layer_indices:\n",
    "        for param in model.bert.encoder.layer[idx].parameters():\n",
    "            param.requires_grad = False\n",
    "    print(f\"Froze BERT layers: {layer_indices}\")\n",
    "\n",
    "\n",
    "def unfreeze_all(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"Unfroze all layers\")\n",
    "\n",
    "\n",
    "# --- Config ---\n",
    "OUTPUT_DIR = \"/content/m2_resume_extractor\"\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "FREEZE_LAYERS = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "FREEZE_EPOCHS = 2\n",
    "TOTAL_EPOCHS = 8\n",
    "\n",
    "# ============================\n",
    "# Phase 1: Frozen layers 0-8\n",
    "# ============================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Phase 1: Frozen layers {FREEZE_LAYERS} for {FREEZE_EPOCHS} epochs\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "freeze_bert_layers(model, FREEZE_LAYERS)\n",
    "\n",
    "phase1_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR + \"/phase1\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=FREEZE_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=phase1_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "phase1_results = trainer.evaluate()\n",
    "print(f\"Phase 1 results: {phase1_results}\")\n",
    "\n",
    "# ============================\n",
    "# Phase 2: All layers unfrozen\n",
    "# ============================\n",
    "remaining_epochs = TOTAL_EPOCHS - FREEZE_EPOCHS\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Phase 2: All layers unfrozen for {remaining_epochs} epochs\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "unfreeze_all(model)\n",
    "\n",
    "phase2_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE * 0.5,  # lower LR for full fine-tuning\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=remaining_epochs,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=phase2_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "results = trainer.evaluate(test_tok)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Precision: {results.get('eval_precision', 0):.4f}\")\n",
    "print(f\"  Recall:    {results.get('eval_recall', 0):.4f}\")\n",
    "print(f\"  F1:        {results.get('eval_f1', 0):.4f}\")\n",
    "\n",
    "TARGET_F1 = 0.90\n",
    "test_f1 = results.get('eval_f1', 0)\n",
    "if test_f1 >= TARGET_F1:\n",
    "    print(f\"\\nTarget F1 {TARGET_F1:.2f} ACHIEVED (got {test_f1:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nTarget F1 {TARGET_F1:.2f} NOT MET (got {test_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save model and download\n",
    "import shutil\n",
    "\n",
    "# Save final model + tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# List saved files\n",
    "for f in sorted(Path(OUTPUT_DIR).iterdir()):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / 1e6\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Zip for download\n",
    "shutil.make_archive(\"/content/m2_resume_extractor_trained\", \"zip\", OUTPUT_DIR)\n",
    "print(f\"\\nZipped to /content/m2_resume_extractor_trained.zip\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download(\"/content/m2_resume_extractor_trained.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Appendix: Using yashpwr Data for Other Models\n\nThe yashpwr dataset (22,855 resumes in chat format) contains rich structured resume text that\ncan be reformatted for other models in the pipeline beyond M2. Below are concrete strategies\nfor each model that could benefit.\n\n---\n\n### M3 (Skills Comparator) - Skill Co-occurrence Pairs\n\n**What M3 needs:** Triplets of (anchor_skill, positive_skill, negative_skill) for contrastive learning.\n\n**How yashpwr helps:** Each resume's Skills section lists skills that co-occur in real professionals.\nSkills listed together on the same resume are implicitly related (positive pairs), while skills\nfrom very different resumes/domains are negatives.\n\n**Data format to generate:**\n```python\n# Extract skills sections from yashpwr resumes\n# For each resume, parse comma-separated skills\n# Build co-occurrence: skills on same resume = positive pair\n# Skills from different industry resumes = negative\ntriplets = []\nfor resume in yashpwr_resumes:\n    skills = extract_skills_section(resume)  # [\"Python\", \"SQL\", \"Machine Learning\"]\n    for i, anchor in enumerate(skills):\n        for j, positive in enumerate(skills):\n            if i != j:\n                negative = random_skill_from_different_domain()\n                triplets.append({\"anchor\": anchor, \"positive\": positive, \"negative\": negative})\n```\n\n**Expected yield:** ~50K-100K skill triplets from real resume co-occurrence patterns.\nThis supplements ESCO/Tabiya synonym data with real-world usage patterns.\n\n---\n\n### M4 (Exp/Edu Comparator) - Resume Feature Extraction\n\n**What M4 needs:** (resume_features, jd_features) pairs with experience/education scores.\n\n**How yashpwr helps:** The assistant message contains a **professional summary** that describes\nthe candidate's experience level, education, and domain. Combined with the structured resume text,\nwe can extract:\n- Years of experience (from the resume text)\n- Education level and field (from Education section)\n- Job titles and seniority progression (from Experience section)\n- Domain/industry (from the summary + job titles)\n\n**Data format to generate:**\n```python\n# For each resume:\n# 1. Extract: years_exp, edu_level, job_titles, skills, domain\n# 2. Generate synthetic JD requirements based on the resume's domain\n# 3. Create pairs with varying match quality (exact match = 1.0, partial = 0.5, mismatch = 0.1)\npairs = []\nfor resume in yashpwr_resumes:\n    features = extract_structured_features(resume)\n    # Positive pair: JD that matches this resume well\n    jd_match = synthesize_matching_jd(features)\n    pairs.append({\"resume\": features, \"jd\": jd_match, \"label\": 0.85 + noise})\n    # Negative pair: JD from different domain\n    jd_mismatch = synthesize_mismatching_jd(features)\n    pairs.append({\"resume\": features, \"jd\": jd_mismatch, \"label\": 0.2 + noise})\n```\n\n**Expected yield:** ~45K pairs (2 per resume) with realistic feature distributions.\n\n---\n\n### M5 (Judge) - Score Calibration Data\n\n**What M5 needs:** Combined scores from M3+M4 mapped to overall match quality.\n\n**How yashpwr helps:** The assistant summaries provide implicit quality signals.\nResumes with clear skill-job alignment in the summary suggest high match scores.\nThis could supplement M5's calibration data after M3/M4 are trained.\n\n**Note:** M5 benefits most AFTER M3 and M4 are trained, as it needs their output scores\nas input features. Save this for a second training pass.\n\n---\n\n### Implementation Priority\n\n| Model | Effort | Impact | Priority |\n|-------|--------|--------|----------|\n| **M2** (done above) | Low | High - adds ~20K sequences | Already implemented |\n| **M3** | Medium | Medium - adds skill co-occurrence | Second priority |\n| **M4** | Medium | Low - already has good data | Nice to have |\n| **M5** | High | Low - needs M3/M4 first | Future work |",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Skills Comparator Training (Colab + T4 GPU)\n",
    "\n",
    "Trains a 256-dim projection head on top of **TechWolf/JobBERT-v2** using contrastive learning with `MultipleNegativesRankingLoss`.\n",
    "\n",
    "## Setup\n",
    "1. Upload `m3_training_data.zip` (created by the packaging cell below, or pre-packaged locally)\n",
    "2. Run all cells\n",
    "3. Download the trained model from `training/models/m3_skills_comparator/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q sentence-transformers datasets pandas pyyaml numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Upload and extract training data\n",
    "# Option A: Upload from Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp /content/drive/MyDrive/m3_training_data.zip /content/\n",
    "\n",
    "# Option B: Upload directly\n",
    "from google.colab import files\n",
    "print(\"Upload m3_training_data.zip...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "!unzip -o m3_training_data.zip -d /content/m3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data preparation - load all skill data sources\n",
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DATA_DIR = Path(\"/content/m3_data\")\n",
    "\n",
    "\n",
    "def load_esco_sentences():\n",
    "    parquet_path = DATA_DIR / \"techwolf_esco_sentences\" / \"train.parquet\"\n",
    "    if not parquet_path.exists():\n",
    "        logger.warning(\"TechWolf ESCO sentences not found -- skipping.\")\n",
    "        return []\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    skill_col, sentence_col = None, None\n",
    "    for col in df.columns:\n",
    "        cl = col.lower()\n",
    "        if \"skill\" in cl and \"sent\" not in cl:\n",
    "            skill_col = col\n",
    "        elif \"sent\" in cl or \"text\" in cl or \"description\" in cl:\n",
    "            sentence_col = col\n",
    "    if skill_col is None or sentence_col is None:\n",
    "        cols = df.columns.tolist()\n",
    "        if len(cols) >= 2:\n",
    "            skill_col, sentence_col = cols[0], cols[1]\n",
    "        else:\n",
    "            return []\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        skill = str(row[skill_col]).strip()\n",
    "        sentence = str(row[sentence_col]).strip()\n",
    "        if skill and sentence and len(skill) > 1:\n",
    "            records.append({\"skill\": skill, \"sentence\": sentence})\n",
    "    logger.info(\"TechWolf ESCO: loaded %d skill-sentence pairs.\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "def load_tabiya_synonyms():\n",
    "    csv_dir = DATA_DIR / \"tabiya_esco\" / \"tabiya-esco-v1.1.1\" / \"csv\"\n",
    "    skills_csv = csv_dir / \"skills.csv\"\n",
    "    if not skills_csv.exists():\n",
    "        # Try alternative paths\n",
    "        for p in DATA_DIR.rglob(\"skills.csv\"):\n",
    "            skills_csv = p\n",
    "            break\n",
    "    if not skills_csv.exists():\n",
    "        logger.warning(\"Tabiya skills.csv not found -- skipping.\")\n",
    "        return {}\n",
    "    df = pd.read_csv(skills_csv)\n",
    "    synonyms = defaultdict(list)\n",
    "    preferred_col, alt_col = None, None\n",
    "    for col in df.columns:\n",
    "        cl = col.lower()\n",
    "        if \"preferred\" in cl and \"label\" in cl:\n",
    "            preferred_col = col\n",
    "        elif \"alt\" in cl and \"label\" in cl:\n",
    "            alt_col = col\n",
    "    if preferred_col is None:\n",
    "        for col in df.columns:\n",
    "            if \"label\" in col.lower() or \"name\" in col.lower():\n",
    "                preferred_col = col\n",
    "                break\n",
    "    if preferred_col is None:\n",
    "        return {}\n",
    "    for _, row in df.iterrows():\n",
    "        pref = str(row[preferred_col]).strip()\n",
    "        if not pref or pref == \"nan\":\n",
    "            continue\n",
    "        alts = []\n",
    "        if alt_col and pd.notna(row.get(alt_col)):\n",
    "            alt_str = str(row[alt_col])\n",
    "            for sep in [\"|\", \"\\n\", \";\"]:\n",
    "                if sep in alt_str:\n",
    "                    alts = [a.strip() for a in alt_str.split(sep) if a.strip()]\n",
    "                    break\n",
    "            if not alts and alt_str.strip():\n",
    "                alts = [alt_str.strip()]\n",
    "        synonyms[pref] = alts\n",
    "    logger.info(\"Tabiya ESCO: loaded %d skills with synonyms.\", len(synonyms))\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def load_nesta_clusters():\n",
    "    nesta_base = DATA_DIR / \"nesta_skills_taxonomy\"\n",
    "    clusters = defaultdict(list)\n",
    "    json_files = list(nesta_base.rglob(\"*.json\")) if nesta_base.exists() else []\n",
    "    csv_files = list(nesta_base.rglob(\"*.csv\")) if nesta_base.exists() else []\n",
    "    if not json_files and not csv_files:\n",
    "        logger.warning(\"Nesta: no data files found -- skipping.\")\n",
    "        return {}\n",
    "    for jf in json_files:\n",
    "        if \"cluster\" in jf.name.lower() or \"taxonomy\" in jf.name.lower():\n",
    "            try:\n",
    "                with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    for key, val in data.items():\n",
    "                        if isinstance(val, list):\n",
    "                            clusters[hash(key) % 1000] = [str(v) for v in val if v]\n",
    "                        elif isinstance(val, dict) and \"skills\" in val:\n",
    "                            clusters[hash(key) % 1000] = [str(s) for s in val[\"skills\"] if s]\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    if not clusters:\n",
    "        for cf in csv_files:\n",
    "            if \"skill\" in cf.name.lower() or \"cluster\" in cf.name.lower():\n",
    "                try:\n",
    "                    df = pd.read_csv(cf)\n",
    "                    skill_col, cluster_col = None, None\n",
    "                    for col in df.columns:\n",
    "                        cl = col.lower()\n",
    "                        if \"skill\" in cl or \"name\" in cl or \"label\" in cl:\n",
    "                            skill_col = col\n",
    "                        elif \"cluster\" in cl or \"group\" in cl or \"category\" in cl:\n",
    "                            cluster_col = col\n",
    "                    if skill_col and cluster_col:\n",
    "                        for _, row in df.iterrows():\n",
    "                            skill = str(row[skill_col]).strip()\n",
    "                            cid = row[cluster_col]\n",
    "                            if skill and skill != \"nan\":\n",
    "                                clusters[int(hash(str(cid)) % 1000)].append(skill)\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "    logger.info(\"Nesta: loaded %d clusters with %d total skills.\", len(clusters), sum(len(v) for v in clusters.values()))\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def load_stacklite_cooccurrence():\n",
    "    tags_path = DATA_DIR / \"stacklite\" / \"question_tags.csv.gz\"\n",
    "    if not tags_path.exists():\n",
    "        logger.warning(\"StackLite tags not found -- skipping.\")\n",
    "        return {}\n",
    "    question_tags = defaultdict(list)\n",
    "    try:\n",
    "        with gzip.open(str(tags_path), \"rt\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for i, row in enumerate(reader):\n",
    "                qid = int(row.get(\"Id\", row.get(\"id\", 0)))\n",
    "                tag = row.get(\"Tag\", row.get(\"tag\", \"\")).strip()\n",
    "                if qid and tag:\n",
    "                    question_tags[qid].append(tag)\n",
    "                if i >= 2_000_000:\n",
    "                    break\n",
    "    except:\n",
    "        return {}\n",
    "    cooccur = defaultdict(set)\n",
    "    for qid, tags in question_tags.items():\n",
    "        for tag in tags:\n",
    "            for other in tags:\n",
    "                if other != tag:\n",
    "                    cooccur[tag].add(other)\n",
    "    logger.info(\"StackLite: built co-occurrence for %d tags.\", len(cooccur))\n",
    "    return cooccur\n",
    "\n",
    "\n",
    "# Load all sources\n",
    "esco_pairs = load_esco_sentences()\n",
    "synonyms = load_tabiya_synonyms()\n",
    "clusters = load_nesta_clusters()\n",
    "cooccur = load_stacklite_cooccurrence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build triplets\n",
    "def build_triplets(esco_pairs, synonyms, clusters, cooccur, max_triplets=500000, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    triplets = []\n",
    "    all_skills = list(set(\n",
    "        list(synonyms.keys())\n",
    "        + [alt for alts in synonyms.values() for alt in alts]\n",
    "        + [skill for skills in clusters.values() for skill in skills]\n",
    "        + list(cooccur.keys())\n",
    "    ))\n",
    "    if len(all_skills) < 10:\n",
    "        logger.warning(\"Insufficient skill vocabulary (%d)\", len(all_skills))\n",
    "        return []\n",
    "\n",
    "    # Synonym-based\n",
    "    for pref, alts in synonyms.items():\n",
    "        for alt in alts:\n",
    "            if alt == pref:\n",
    "                continue\n",
    "            neg = rng.choice(all_skills)\n",
    "            while neg == pref or neg == alt:\n",
    "                neg = rng.choice(all_skills)\n",
    "            triplets.append({\"anchor\": pref, \"positive\": alt, \"negative\": neg})\n",
    "            if len(triplets) >= max_triplets:\n",
    "                break\n",
    "        if len(triplets) >= max_triplets:\n",
    "            break\n",
    "\n",
    "    # Cluster-based\n",
    "    cluster_list = list(clusters.items())\n",
    "    all_cluster_skills = [s for skills in clusters.values() for s in skills]\n",
    "    if cluster_list and all_cluster_skills:\n",
    "        for cid, skills in cluster_list:\n",
    "            if len(skills) < 2:\n",
    "                continue\n",
    "            other_cluster_skills = [s for oid, oss in cluster_list if oid != cid for s in oss]\n",
    "            if not other_cluster_skills:\n",
    "                other_cluster_skills = all_skills\n",
    "            for i in range(len(skills)):\n",
    "                for j in range(i + 1, min(i + 3, len(skills))):\n",
    "                    anchor = skills[i]\n",
    "                    positive = skills[j]\n",
    "                    negative = rng.choice(other_cluster_skills)\n",
    "                    triplets.append({\"anchor\": anchor, \"positive\": positive, \"negative\": negative})\n",
    "                    if len(triplets) >= max_triplets:\n",
    "                        break\n",
    "                if len(triplets) >= max_triplets:\n",
    "                    break\n",
    "            if len(triplets) >= max_triplets:\n",
    "                break\n",
    "\n",
    "    # ESCO sentence-based\n",
    "    skill_to_sentences = defaultdict(list)\n",
    "    for pair in esco_pairs:\n",
    "        skill_to_sentences[pair[\"skill\"]].append(pair[\"sentence\"])\n",
    "    skill_keys = list(skill_to_sentences.keys())\n",
    "    if len(skill_keys) >= 2 and len(triplets) < max_triplets:\n",
    "        for skill in skill_keys:\n",
    "            sentences = skill_to_sentences[skill]\n",
    "            if len(sentences) < 2:\n",
    "                continue\n",
    "            for i in range(min(len(sentences), 3)):\n",
    "                for j in range(i + 1, min(len(sentences), 4)):\n",
    "                    neg_skill = rng.choice(skill_keys)\n",
    "                    while neg_skill == skill:\n",
    "                        neg_skill = rng.choice(skill_keys)\n",
    "                    neg_sentences = skill_to_sentences[neg_skill]\n",
    "                    neg_sentence = rng.choice(neg_sentences) if neg_sentences else neg_skill\n",
    "                    triplets.append({\"anchor\": sentences[i], \"positive\": sentences[j], \"negative\": neg_sentence})\n",
    "                    if len(triplets) >= max_triplets:\n",
    "                        break\n",
    "                if len(triplets) >= max_triplets:\n",
    "                    break\n",
    "            if len(triplets) >= max_triplets:\n",
    "                break\n",
    "\n",
    "    rng.shuffle(triplets)\n",
    "    triplets = triplets[:max_triplets]\n",
    "    logger.info(\"Built %d triplets.\", len(triplets))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def create_hard_negatives(triplets, cooccur, fraction=0.3, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    if not cooccur or not triplets:\n",
    "        return triplets\n",
    "    n_replace = int(len(triplets) * fraction)\n",
    "    indices = rng.sample(range(len(triplets)), min(n_replace, len(triplets)))\n",
    "    replaced = 0\n",
    "    for idx in indices:\n",
    "        anchor = triplets[idx][\"anchor\"]\n",
    "        positive = triplets[idx][\"positive\"]\n",
    "        cooccurring = cooccur.get(anchor, set()) - {positive, anchor}\n",
    "        if not cooccurring:\n",
    "            cooccurring = cooccur.get(anchor.lower(), set()) - {positive.lower(), anchor.lower()}\n",
    "        if cooccurring:\n",
    "            triplets[idx][\"negative\"] = rng.choice(list(cooccurring))\n",
    "            replaced += 1\n",
    "    logger.info(\"Replaced %d / %d negatives with hard negatives.\", replaced, len(triplets))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "triplets = build_triplets(esco_pairs, synonyms, clusters, cooccur, max_triplets=500000)\n",
    "triplets = create_hard_negatives(triplets, cooccur)\n",
    "print(f\"Total triplets: {len(triplets)}\")\n",
    "print(f\"Sample: {triplets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train the model\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Config\n",
    "BASE_MODEL = \"TechWolf/JobBERT-v2\"\n",
    "PROJECTION_DIM = 256\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "WARMUP_RATIO = 0.1\n",
    "FP16 = True\n",
    "OUTPUT_DIR = \"/content/m3_skills_comparator\"\n",
    "\n",
    "# Load model + add projection head\n",
    "model = SentenceTransformer(BASE_MODEL)\n",
    "projection = models.Dense(\n",
    "    in_features=model.get_sentence_embedding_dimension(),\n",
    "    out_features=PROJECTION_DIM,\n",
    "    activation_function=None,\n",
    ")\n",
    "model.add_module(\"projection\", projection)\n",
    "print(f\"Model loaded. Embedding dim: {model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Split triplets\n",
    "np.random.shuffle(triplets)\n",
    "split = int(len(triplets) * 0.9)\n",
    "train_triplets = triplets[:split]\n",
    "eval_triplets = triplets[split:]\n",
    "\n",
    "# DataLoader\n",
    "train_examples = [\n",
    "    InputExample(texts=[t[\"anchor\"], t[\"positive\"], t[\"negative\"]])\n",
    "    for t in train_triplets\n",
    "]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Loss & evaluator\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "evaluator = TripletEvaluator(\n",
    "    anchors=[t[\"anchor\"] for t in eval_triplets],\n",
    "    positives=[t[\"positive\"] for t in eval_triplets],\n",
    "    negatives=[t[\"negative\"] for t in eval_triplets],\n",
    "    name=\"skill_triplet_eval\",\n",
    ")\n",
    "\n",
    "warmup_steps = int(len(train_dataloader) * EPOCHS * WARMUP_RATIO)\n",
    "\n",
    "print(f\"Training: {len(train_triplets)} train, {len(eval_triplets)} eval\")\n",
    "print(f\"Batches/epoch: {len(train_dataloader)}, warmup: {warmup_steps} steps\")\n",
    "print(f\"Starting training...\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=OUTPUT_DIR,\n",
    "    evaluation_steps=len(train_dataloader) // 2,\n",
    "    save_best_model=True,\n",
    "    use_amp=FP16,\n",
    ")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Export projection weights & evaluate\n",
    "projection_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if \"projection\" in name:\n",
    "        projection_weights[name.split(\".\")[-1]] = param.detach().cpu().numpy()\n",
    "\n",
    "projection_path = Path(OUTPUT_DIR) / \"projection.npy\"\n",
    "np.save(str(projection_path), projection_weights)\n",
    "print(f\"Projection weights saved to {projection_path}\")\n",
    "\n",
    "# Final evaluation\n",
    "eval_score = evaluator(model, output_path=OUTPUT_DIR)\n",
    "TARGET = 0.80\n",
    "if eval_score >= TARGET:\n",
    "    print(f\"Target accuracy {TARGET:.2f} ACHIEVED (got {eval_score:.4f})\")\n",
    "else:\n",
    "    print(f\"Target accuracy {TARGET:.2f} NOT MET (got {eval_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Download trained model\n",
    "import shutil\n",
    "\n",
    "# Zip the model directory\n",
    "shutil.make_archive(\"/content/m3_skills_comparator_trained\", \"zip\", OUTPUT_DIR)\n",
    "print(f\"Model zipped to /content/m3_skills_comparator_trained.zip\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download(\"/content/m3_skills_comparator_trained.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

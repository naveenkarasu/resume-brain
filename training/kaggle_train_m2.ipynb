{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# M2 Resume Extractor Training (Kaggle + P100 GPU)\n\nFine-tunes **yashpwr/resume-ner-bert-v2** for BIO NER on resumes with 14 entity types (29 labels).\nTwo-phase training: frozen layers 0-8 for 2 epochs, then all layers unfrozen for 6 epochs.\n\n## Setup\n1. Upload `m2_training_data.zip` as a Kaggle Dataset (it will auto-extract)\n2. Add the dataset to this notebook via the sidebar **Add Data** button\n3. Select **GPU P100** in notebook settings (Settings > Accelerator)\n4. Enable **Internet** (Settings > Internet > On)\n5. Run all cells\n6. Download the trained model zip from the output\n\n## Data Sources\n| Dataset | Type | Expected Sequences |\n|---------|------|-------------------|\n| Mehyaar | Gold NER annotations | ~25K |\n| DataTurks | Gold NER annotations | ~1K |\n| Djinni | Weak supervision from structured fields | ~16K |\n| DatasetMaster | Structured field synthesis | varies |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Suppress TF/CUDA warnings + install dependencies\n#\n# Kaggle pre-loads both TensorFlow and PyTorch, causing duplicate CUDA factory\n# registration messages (cuFFT, cuDNN, cuBLAS). These are cosmetic only and do\n# NOT affect training. Setting TF_CPP_MIN_LOG_LEVEL=3 before any imports\n# suppresses them entirely.\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'           # Suppress TF C++ logs (INFO/WARNING/ERROR)\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'           # Suppress oneDNN messages\nos.environ['GRPC_VERBOSITY'] = 'ERROR'               # Suppress gRPC logs\nos.environ['ABSL_MIN_LOG_LEVEL'] = '2'               # Suppress abseil warnings\n\nimport warnings\nwarnings.filterwarnings('ignore', message='.*computation placer already registered.*')\nwarnings.filterwarnings('ignore', message='.*Unable to register.*factory.*')\n\n!pip install -q transformers datasets seqeval accelerate pandas pyarrow pyyaml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Enable GPU in Settings > Accelerator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Locate training data\n",
    "#\n",
    "# Kaggle auto-extracts uploaded zip files. When you upload m2_training_data.zip\n",
    "# as a dataset, the contents are extracted into /kaggle/input/<dataset-name>/.\n",
    "# Kaggle converts underscores to hyphens in dataset names, so \"m2_training_data\"\n",
    "# becomes \"/kaggle/input/m2-training-data/\". The actual folder structure may be\n",
    "# nested further depending on how the zip was created.\n",
    "#\n",
    "# This cell searches /kaggle/input/ for the expected data folders and sets DATA_DIR.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The 5 expected data subdirectories\n",
    "TARGET_FOLDERS = {\n",
    "    \"yashpwr_resume_ner\", \"dataturks_resume_ner\", \"mehyaar_ner_cvs\",\n",
    "    \"datasetmaster_resumes\", \"djinni_candidates\"\n",
    "}\n",
    "\n",
    "print(\"Available datasets in /kaggle/input/:\")\n",
    "for d in os.listdir(\"/kaggle/input/\"):\n",
    "    print(f\"  /kaggle/input/{d}/\")\n",
    "\n",
    "# Search for the directory containing our data folders\n",
    "DATA_DIR = None\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "    if any(d in TARGET_FOLDERS for d in dirs):\n",
    "        DATA_DIR = Path(root)\n",
    "        break\n",
    "    # Don't search too deep\n",
    "    if root.count(os.sep) - \"/kaggle/input/\".count(os.sep) > 4:\n",
    "        break\n",
    "\n",
    "if DATA_DIR:\n",
    "    print(f\"\\nData root found: {DATA_DIR}\")\n",
    "    print(\"Contents:\")\n",
    "    for item in sorted(os.listdir(DATA_DIR)):\n",
    "        full = DATA_DIR / item\n",
    "        if full.is_dir():\n",
    "            count = sum(1 for _ in full.rglob(\"*\") if _.is_file())\n",
    "            print(f\"  {item}/ ({count} files)\")\n",
    "        else:\n",
    "            print(f\"  {item} ({full.stat().st_size / 1e6:.1f} MB)\")\n",
    "else:\n",
    "    # Fallback: show full tree for debugging\n",
    "    print(\"\\nERROR: Could not find expected data folders!\")\n",
    "    print(\"Full /kaggle/input/ tree:\")\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "        depth = root.replace(\"/kaggle/input/\", \"\").count(os.sep)\n",
    "        indent = \"  \" * depth\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        if depth < 4:\n",
    "            for f in files[:10]:\n",
    "                print(f\"{indent}  {f}\")\n",
    "            if len(files) > 10:\n",
    "                print(f\"{indent}  ... and {len(files) - 10} more files\")\n",
    "    print(\"\\nPlease add m2_training_data.zip as a Dataset in the notebook sidebar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Data preparation - load and unify all resume NER datasets\n\nimport json\nimport logging\nimport random\nimport re\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict, Features, Sequence, Value\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# DATA_DIR is set by Cell 3\nassert DATA_DIR is not None, \"DATA_DIR not set! Re-run Cell 3.\"\nprint(f\"Using DATA_DIR: {DATA_DIR}\")\n\n# --- Entity types and BIO labels ---\nENTITY_TYPES = [\n    \"NAME\", \"EMAIL\", \"PHONE\", \"LOCATION\", \"DESIGNATION\", \"COMPANY\",\n    \"DEGREE\", \"GRADUATION_YEAR\", \"COLLEGE_NAME\", \"YEARS_OF_EXPERIENCE\",\n    \"SKILLS\", \"CERTIFICATION\", \"PROJECT_NAME\", \"PROJECT_TECHNOLOGY\",\n]\n\nLABELS = [\"O\"]\nfor etype in ENTITY_TYPES:\n    LABELS.append(f\"B-{etype}\")\n    LABELS.append(f\"I-{etype}\")\n\nLABEL2ID = {label: idx for idx, label in enumerate(LABELS)}\nID2LABEL = {idx: label for idx, label in enumerate(LABELS)}\n\nprint(f\"Entity types: {len(ENTITY_TYPES)}\")\nprint(f\"Total labels (BIO): {len(LABELS)}\")\n\n# --- Label normalization map ---\n_LABEL_NORMALIZE = {\n    \"Name\": \"NAME\", \"name\": \"NAME\",\n    \"EMAIL\": \"EMAIL\", \"Email Address\": \"EMAIL\", \"email\": \"EMAIL\",\n    \"Phone\": \"PHONE\", \"phone\": \"PHONE\", \"PHONE\": \"PHONE\",\n    \"Location\": \"LOCATION\", \"location\": \"LOCATION\", \"LOCATION\": \"LOCATION\",\n    \"Designation\": \"DESIGNATION\", \"designation\": \"DESIGNATION\", \"DESIGNATION\": \"DESIGNATION\",\n    \"Companies worked at\": \"COMPANY\", \"Company\": \"COMPANY\", \"company\": \"COMPANY\", \"COMPANY\": \"COMPANY\",\n    \"Degree\": \"DEGREE\", \"degree\": \"DEGREE\", \"DEGREE\": \"DEGREE\",\n    \"Graduation Year\": \"GRADUATION_YEAR\", \"graduation_year\": \"GRADUATION_YEAR\",\n    \"College Name\": \"COLLEGE_NAME\", \"college_name\": \"COLLEGE_NAME\", \"COLLEGE\": \"COLLEGE_NAME\",\n    \"Years of Experience\": \"YEARS_OF_EXPERIENCE\", \"years_of_experience\": \"YEARS_OF_EXPERIENCE\", \"Experience\": \"YEARS_OF_EXPERIENCE\",\n    \"Skills\": \"SKILLS\", \"skills\": \"SKILLS\", \"SKILLS\": \"SKILLS\",\n    \"Certification\": \"CERTIFICATION\", \"certification\": \"CERTIFICATION\", \"CERTIFICATION\": \"CERTIFICATION\",\n    \"Project\": \"PROJECT_NAME\", \"project\": \"PROJECT_NAME\", \"PROJECT\": \"PROJECT_NAME\",\n    \"Technology\": \"PROJECT_TECHNOLOGY\", \"technology\": \"PROJECT_TECHNOLOGY\",\n}\n\n\ndef _normalize_label(raw_label):\n    return _LABEL_NORMALIZE.get(raw_label)\n\n\ndef _tokenize_with_offsets(text):\n    tokens, offsets = [], []\n    for m in re.finditer(r\"\\S+\", text):\n        tokens.append(m.group())\n        offsets.append((m.start(), m.end()))\n    return tokens, offsets\n\n\ndef _bio_tags_from_char_spans(tokens, char_offsets, spans):\n    tags = [\"O\"] * len(tokens)\n    for span in spans:\n        s_start, s_end, label = span[\"start\"], span[\"end\"], span[\"label\"]\n        first = True\n        for idx, (t_start, t_end) in enumerate(char_offsets):\n            if t_end <= s_start or t_start >= s_end:\n                continue\n            prefix = \"B\" if first else \"I\"\n            tag = f\"{prefix}-{label}\"\n            if tag in LABEL2ID:\n                tags[idx] = tag\n                first = False\n    return tags\n\n\ndef _label_to_id(tags):\n    return [LABEL2ID.get(t, 0) for t in tags]\n\n\n# --- Dataset Loaders ---\n\ndef load_dataturks():\n    json_path = DATA_DIR / \"dataturks_resume_ner\" / \"Entity Recognition in Resumes.json\"\n    if not json_path.exists():\n        logger.warning(\"DataTurks not found -- skipping.\")\n        return []\n    records = []\n    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                entry = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            content = entry.get(\"content\", \"\")\n            annotations = entry.get(\"annotation\", [])\n            if not content or not annotations:\n                continue\n            tokens, offsets = _tokenize_with_offsets(content)\n            if len(tokens) < 3:\n                continue\n            spans = []\n            for ann in annotations:\n                raw_labels = ann.get(\"label\", [])\n                if isinstance(raw_labels, str):\n                    raw_labels = [raw_labels]\n                points = ann.get(\"points\", [])\n                if not points:\n                    continue\n                for raw_label in raw_labels:\n                    norm = _normalize_label(raw_label)\n                    if norm is None:\n                        continue\n                    for pt in points:\n                        start, end = pt.get(\"start\"), pt.get(\"end\")\n                        if start is None or end is None:\n                            continue\n                        spans.append({\"start\": start, \"end\": end + 1, \"label\": norm})\n            tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n            for i in range(0, len(tokens), 128):\n                ct, ctags = tokens[i:i+128], tags[i:i+128]\n                if len(ct) >= 3:\n                    records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"dataturks\"})\n    logger.info(\"DataTurks: %d sequences\", len(records))\n    return records\n\n\ndef load_mehyaar():\n    base_dir = DATA_DIR / \"mehyaar_ner_cvs\" / \"ResumesJsonAnnotated\" / \"ResumesJsonAnnotated\"\n    if not base_dir.exists():\n        logger.warning(\"Mehyaar not found -- skipping.\")\n        return []\n    records = []\n    for jf in sorted(base_dir.glob(\"*.json\")):\n        try:\n            with open(jf, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            continue\n        text = data.get(\"text\", data.get(\"content\", \"\"))\n        annotations = data.get(\"annotations\", data.get(\"annotation\", []))\n        if not text or not annotations:\n            continue\n        tokens, offsets = _tokenize_with_offsets(text)\n        if len(tokens) < 3:\n            continue\n        spans = []\n        for ann in annotations:\n            if isinstance(ann, dict):\n                label = ann.get(\"label\", ann.get(\"type\", \"\"))\n                start = ann.get(\"start\", ann.get(\"startOffset\"))\n                end = ann.get(\"end\", ann.get(\"endOffset\"))\n            elif isinstance(ann, (list, tuple)) and len(ann) >= 3:\n                start, end, label = ann[0], ann[1], ann[2]\n            else:\n                continue\n            if start is None or end is None or not label:\n                continue\n            norm = _normalize_label(label)\n            if norm is None:\n                if label.upper() in [e.upper() for e in ENTITY_TYPES]:\n                    norm = label.upper()\n                else:\n                    continue\n            spans.append({\"start\": int(start), \"end\": int(end), \"label\": norm})\n        tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n        for i in range(0, len(tokens), 128):\n            ct, ctags = tokens[i:i+128], tags[i:i+128]\n            if len(ct) >= 3:\n                records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"mehyaar\"})\n    logger.info(\"Mehyaar: %d sequences\", len(records))\n    return records\n\n\ndef load_datasetmaster():\n    parquet_path = DATA_DIR / \"datasetmaster_resumes\" / \"train.parquet\"\n    if not parquet_path.exists():\n        logger.warning(\"DatasetMaster not found -- skipping.\")\n        return []\n    df = pd.read_parquet(parquet_path)\n    records = []\n    text_col = None\n    for candidate in [\"resume_text\", \"text\", \"content\", \"resume\", \"Resume\"]:\n        if candidate in df.columns:\n            text_col = candidate\n            break\n    if text_col is None:\n        for _, row in df.iterrows():\n            tokens_all, tags_all = [], []\n            for col, label in [(\"skills\", \"SKILLS\"), (\"education\", \"DEGREE\"), (\"projects\", \"PROJECT_NAME\")]:\n                val = row.get(col)\n                if isinstance(val, str) and val.strip():\n                    toks, _ = _tokenize_with_offsets(val)\n                    tokens_all.extend(toks)\n                    tags_all.extend([f\"B-{label}\"] + [f\"I-{label}\"] * (len(toks) - 1))\n                elif isinstance(val, list):\n                    for item in val:\n                        if isinstance(item, str) and item.strip():\n                            toks, _ = _tokenize_with_offsets(item)\n                            tokens_all.extend(toks)\n                            tags_all.extend([f\"B-{label}\"] + [f\"I-{label}\"] * (len(toks) - 1))\n            if len(tokens_all) >= 5:\n                records.append({\"tokens\": tokens_all, \"ner_tags\": _label_to_id(tags_all), \"source\": \"datasetmaster\"})\n        logger.info(\"DatasetMaster (structured): %d sequences\", len(records))\n        return records\n    field_label_map = {\n        \"skills\": \"SKILLS\", \"education\": \"DEGREE\", \"company\": \"COMPANY\",\n        \"designation\": \"DESIGNATION\", \"college\": \"COLLEGE_NAME\", \"degree\": \"DEGREE\",\n        \"projects\": \"PROJECT_NAME\", \"certification\": \"CERTIFICATION\", \"certifications\": \"CERTIFICATION\",\n    }\n    for _, row in df.iterrows():\n        text = row[text_col]\n        if not isinstance(text, str) or len(text) < 30:\n            continue\n        tokens, offsets = _tokenize_with_offsets(text[:2000])\n        if len(tokens) < 5:\n            continue\n        spans = []\n        for col, label in field_label_map.items():\n            val = row.get(col)\n            if val is None:\n                continue\n            search_terms = []\n            if isinstance(val, str) and val.strip():\n                search_terms = [val.strip()]\n            elif isinstance(val, list):\n                search_terms = [str(v).strip() for v in val if isinstance(v, str) and v.strip()]\n            for term in search_terms[:10]:\n                try:\n                    for m in re.finditer(re.escape(term[:100]), text[:2000], re.IGNORECASE):\n                        spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": label})\n                        break\n                except re.error:\n                    continue\n        tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n        for i in range(0, len(tokens), 128):\n            ct, ctags = tokens[i:i+128], tags[i:i+128]\n            if len(ct) >= 3:\n                records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"datasetmaster\"})\n    logger.info(\"DatasetMaster: %d sequences\", len(records))\n    return records\n\n\ndef load_djinni():\n    \"\"\"Load Djinni candidates. Uses 'CV' column for text, 'Position' for designation,\n    'Primary Keyword' for skills.\"\"\"\n    parquet_path = DATA_DIR / \"djinni_candidates\" / \"train.parquet\"\n    if not parquet_path.exists():\n        logger.warning(\"Djinni not found -- skipping.\")\n        return []\n    df = pd.read_parquet(parquet_path)\n    if len(df) > 15000:\n        df = df.sample(n=15000, random_state=42)\n    records = []\n    exp_pattern = re.compile(r\"\\b(\\d+)\\+?\\s*years?\\b\", re.IGNORECASE)\n\n    for _, row in df.iterrows():\n        text = \"\"\n        for col in [\"CV\", \"Moreinfo\", \"Highlights\", \"description\", \"text\", \"bio\", \"summary\", \"content\"]:\n            val = row.get(col, \"\")\n            if isinstance(val, str) and len(val) > 20:\n                text = val\n                break\n        if not text:\n            continue\n\n        text = text[:1500]\n        tokens, offsets = _tokenize_with_offsets(text)\n        if len(tokens) < 5:\n            continue\n\n        spans = []\n\n        for m in exp_pattern.finditer(text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"YEARS_OF_EXPERIENCE\"})\n\n        for skill_col in [\"Primary Keyword\", \"skills\", \"keywords\", \"technologies\"]:\n            skills_val = row.get(skill_col, \"\")\n            if isinstance(skills_val, str) and skills_val.strip():\n                skill_list = [s.strip() for s in skills_val.split(\",\") if s.strip()]\n                for skill in skill_list[:15]:\n                    try:\n                        for m in re.finditer(re.escape(skill), text, re.IGNORECASE):\n                            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"SKILLS\"})\n                            break\n                    except re.error:\n                        continue\n                break\n\n        for pos_col in [\"Position\", \"position\", \"title\", \"designation\", \"job_title\"]:\n            pos_val = row.get(pos_col, \"\")\n            if isinstance(pos_val, str) and pos_val.strip():\n                try:\n                    for m in re.finditer(re.escape(pos_val.strip()), text, re.IGNORECASE):\n                        spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"DESIGNATION\"})\n                        break\n                except re.error:\n                    pass\n                break\n\n        tags = _bio_tags_from_char_spans(tokens, offsets, spans)\n        if any(t != \"O\" for t in tags):\n            for i in range(0, len(tokens), 128):\n                ct, ctags = tokens[i:i+128], tags[i:i+128]\n                if len(ct) >= 3:\n                    records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"djinni\"})\n\n    logger.info(\"Djinni: %d sequences\", len(records))\n    return records\n\n\n# --- Load all datasets ---\nprint(\"Loading datasets...\")\nall_records = []\nfor name, loader in [(\"DataTurks\", load_dataturks), (\"Mehyaar\", load_mehyaar),\n                      (\"DatasetMaster\", load_datasetmaster), (\"Djinni\", load_djinni)]:\n    try:\n        recs = loader()\n        all_records.extend(recs)\n        print(f\"  {name}: {len(recs)} sequences (total: {len(all_records)})\")\n    except Exception as e:\n        print(f\"  {name}: FAILED - {e}\")\n\nfor rec in all_records:\n    rec.pop(\"_raw_tags\", None)\n\nprint(f\"\\nTotal: {len(all_records)} sequences\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Clean and split data\n\ncleaned = []\nfor rec in all_records:\n    tokens = [str(t) if not isinstance(t, str) else t for t in rec.get(\"tokens\", [])]\n    tags = [int(t) if not isinstance(t, int) else t for t in rec.get(\"ner_tags\", [])]\n    if not tokens or len(tokens) != len(tags):\n        continue\n    if any(t in (\"nan\", \"None\", \"\") for t in tokens):\n        continue\n    tokens = [t.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\") for t in tokens]\n    source = str(rec.get(\"source\", \"unknown\")).encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")\n    cleaned.append({\"tokens\": tokens, \"ner_tags\": tags, \"source\": source})\n\nprint(f\"Cleaned: {len(all_records)} -> {len(cleaned)} records (dropped {len(all_records) - len(cleaned)})\")\n\nTRAIN_RATIO = 0.8\nVAL_RATIO = 0.1\n\nrng = random.Random(42)\nrng.shuffle(cleaned)\n\nn = len(cleaned)\nn_train = int(n * TRAIN_RATIO)\nn_val = int(n * VAL_RATIO)\n\nsplits = {\n    \"train\": cleaned[:n_train],\n    \"validation\": cleaned[n_train:n_train + n_val],\n    \"test\": cleaned[n_train + n_val:],\n}\n\nfeatures = Features({\n    \"tokens\": Sequence(Value(\"string\")),\n    \"ner_tags\": Sequence(Value(\"int32\")),\n    \"source\": Value(\"string\"),\n})\n\ndd = DatasetDict()\nfor split_name, split_records in splits.items():\n    dd[split_name] = Dataset.from_dict(\n        {\n            \"tokens\": [r[\"tokens\"] for r in split_records],\n            \"ner_tags\": [r[\"ner_tags\"] for r in split_records],\n            \"source\": [r[\"source\"] for r in split_records],\n        },\n        features=features,\n    )\n    print(f\"  {split_name}: {len(split_records)} examples\")\n\ntrain_ds, val_ds, test_ds = dd[\"train\"], dd[\"validation\"], dd[\"test\"]\nprint(f\"\\nTrain: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Tokenize and align labels\nfrom transformers import AutoTokenizer\n\nBASE_MODEL = \"yashpwr/resume-ner-bert-v2\"\nMAX_LENGTH = 512\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\ndef tokenize_and_align(examples):\n    tokenized = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n    )\n    all_labels = []\n    for i, labels in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized.word_ids(batch_index=i)\n        aligned = []\n        previous_word_id = None\n        for word_id in word_ids:\n            if word_id is None:\n                aligned.append(-100)\n            elif word_id != previous_word_id:\n                aligned.append(labels[word_id])\n            else:\n                aligned.append(-100)\n            previous_word_id = word_id\n        all_labels.append(aligned)\n    tokenized[\"labels\"] = all_labels\n    return tokenized\n\nprint(\"Tokenizing train...\")\ntrain_tok = train_ds.map(tokenize_and_align, batched=True, remove_columns=train_ds.column_names)\nprint(\"Tokenizing val...\")\nval_tok = val_ds.map(tokenize_and_align, batched=True, remove_columns=val_ds.column_names)\nprint(\"Tokenizing test...\")\ntest_tok = test_ds.map(tokenize_and_align, batched=True, remove_columns=test_ds.column_names)\nprint(f\"Tokenized: train={len(train_tok)}, val={len(val_tok)}, test={len(test_tok)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Model setup + two-phase training\n\nimport transformers\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nfrom seqeval.metrics import f1_score, precision_score, recall_score\n\nlabel_list = LABELS\nid2label = ID2LABEL\nlabel2id = LABEL2ID\n\n# Temporarily suppress transformers warnings during model load.\n# The base model has 25 labels (11 entity types) but we need 29 (14 entity types).\n# The classifier head gets re-initialized with the correct size - this is intentional.\ntransformers.logging.set_verbosity_error()\nmodel = AutoModelForTokenClassification.from_pretrained(\n    BASE_MODEL,\n    num_labels=len(label_list),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\ntransformers.logging.set_verbosity_warning()\nprint(f\"Model loaded: {BASE_MODEL} with {len(label_list)} labels (classifier head re-initialized)\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=2)\n    true_labels, true_preds = [], []\n    for pred_seq, label_seq in zip(predictions, labels):\n        t_labels, t_preds = [], []\n        for p, l in zip(pred_seq, label_seq):\n            if l == -100:\n                continue\n            t_labels.append(label_list[l])\n            t_preds.append(label_list[p])\n        true_labels.append(t_labels)\n        true_preds.append(t_preds)\n    return {\n        \"precision\": precision_score(true_labels, true_preds),\n        \"recall\": recall_score(true_labels, true_preds),\n        \"f1\": f1_score(true_labels, true_preds),\n    }\n\n\ndef freeze_bert_layers(model, layer_indices):\n    for idx in layer_indices:\n        for param in model.bert.encoder.layer[idx].parameters():\n            param.requires_grad = False\n    print(f\"Froze BERT layers: {layer_indices}\")\n\n\ndef unfreeze_all(model):\n    for param in model.parameters():\n        param.requires_grad = True\n    print(\"Unfroze all layers\")\n\n\n# --- Config ---\nOUTPUT_DIR = \"/kaggle/working/m2_resume_extractor\"\nLEARNING_RATE = 2e-5\nBATCH_SIZE = 16\nWEIGHT_DECAY = 0.01\nWARMUP_RATIO = 0.1\nFREEZE_LAYERS = [0, 1, 2, 3, 4, 5, 6, 7, 8]\nFREEZE_EPOCHS = 2\nTOTAL_EPOCHS = 8\n\n# ============================\n# Phase 1: Frozen layers 0-8\n# ============================\nprint(f\"\\n{'='*60}\")\nprint(f\"Phase 1: Frozen layers {FREEZE_LAYERS} for {FREEZE_EPOCHS} epochs\")\nprint(f\"{'='*60}\")\n\nfreeze_bert_layers(model, FREEZE_LAYERS)\n\nphase1_args = TrainingArguments(\n    output_dir=OUTPUT_DIR + \"/phase1\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=FREEZE_EPOCHS,\n    weight_decay=WEIGHT_DECAY,\n    warmup_ratio=WARMUP_RATIO,\n    fp16=True,\n    logging_steps=50,\n    save_total_limit=1,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=phase1_args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nphase1_results = trainer.evaluate()\nprint(f\"Phase 1 results: {phase1_results}\")\n\n# ============================\n# Phase 2: All layers unfrozen\n# ============================\nremaining_epochs = TOTAL_EPOCHS - FREEZE_EPOCHS\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Phase 2: All layers unfrozen for {remaining_epochs} epochs\")\nprint(f\"{'='*60}\")\n\nunfreeze_all(model)\n\nphase2_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=LEARNING_RATE * 0.5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=remaining_epochs,\n    weight_decay=WEIGHT_DECAY,\n    warmup_ratio=WARMUP_RATIO,\n    fp16=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=phase2_args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Evaluate on test set\nprint(\"Evaluating on test set...\")\nresults = trainer.evaluate(test_tok)\nprint(f\"\\nTest Results:\")\nprint(f\"  Precision: {results.get('eval_precision', 0):.4f}\")\nprint(f\"  Recall:    {results.get('eval_recall', 0):.4f}\")\nprint(f\"  F1:        {results.get('eval_f1', 0):.4f}\")\n\nTARGET_F1 = 0.90\ntest_f1 = results.get('eval_f1', 0)\nif test_f1 >= TARGET_F1:\n    print(f\"\\nTarget F1 {TARGET_F1:.2f} ACHIEVED (got {test_f1:.4f})\")\nelse:\n    print(f\"\\nTarget F1 {TARGET_F1:.2f} NOT MET (got {test_f1:.4f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Save model and download\nimport shutil\n\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Model saved to {OUTPUT_DIR}\")\n\n# List saved files\nfor f in sorted(Path(OUTPUT_DIR).iterdir()):\n    if f.is_file():\n        size_mb = f.stat().st_size / 1e6\n        print(f\"  {f.name}: {size_mb:.1f} MB\")\n\n# Zip for download\nshutil.make_archive(\"/kaggle/working/m2_resume_extractor_trained\", \"zip\", OUTPUT_DIR)\nzip_size = Path(\"/kaggle/working/m2_resume_extractor_trained.zip\").stat().st_size / 1e6\nprint(f\"\\nZipped to /kaggle/working/m2_resume_extractor_trained.zip ({zip_size:.1f} MB)\")\nprint(\"\\nDownload from the Output tab on the right sidebar.\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Optional: yashpwr Weak Supervision NER\n\n**Skip this section unless you want to add ~20K extra weakly-labeled sequences.**\n\nThe yashpwr dataset has a `messages` column (chat/instruction format) instead of BIO-tagged\n`tokens`/`ner_tags`. This cell extracts resume text from the user message and applies regex-based\nweak supervision to auto-generate BIO tags.\n\n**To use:** Run the code cell below BEFORE Cell 5 (Clean & split), then re-run Cells 5-9.\n\n**Entity detection:** EMAIL, PHONE, LOCATION, DEGREE, SKILLS (section-aware), DESIGNATION,\nCOMPANY, COLLEGE_NAME, GRADUATION_YEAR, CERTIFICATION, YEARS_OF_EXPERIENCE.\n\n**Label quality:** ~70-80% accuracy. Mixing with gold-standard data improves entity coverage.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Using yashpwr Data for Other Models\n",
    "\n",
    "The yashpwr dataset (22,855 resumes in chat format) contains rich structured resume text that\n",
    "can be reformatted for other models in the pipeline beyond M2.\n",
    "\n",
    "---\n",
    "\n",
    "### M3 (Skills Comparator) - Skill Co-occurrence Pairs\n",
    "\n",
    "**What M3 needs:** Triplets of (anchor_skill, positive_skill, negative_skill) for contrastive learning.\n",
    "\n",
    "**How yashpwr helps:** Each resume's Skills section lists skills that co-occur in real professionals.\n",
    "Skills listed together = positive pair. Skills from different domains = negative.\n",
    "\n",
    "```python\n",
    "# Pseudocode: extract skill co-occurrence from yashpwr\n",
    "triplets = []\n",
    "for resume in yashpwr_resumes:\n",
    "    skills = extract_skills_section(resume)  # [\"Python\", \"SQL\", \"ML\"]\n",
    "    for i, anchor in enumerate(skills):\n",
    "        for j, positive in enumerate(skills):\n",
    "            if i != j:\n",
    "                negative = random_skill_from_different_domain()\n",
    "                triplets.append({\"anchor\": anchor, \"positive\": positive, \"negative\": negative})\n",
    "```\n",
    "\n",
    "**Expected yield:** ~50K-100K skill triplets from real resume co-occurrence.\n",
    "\n",
    "---\n",
    "\n",
    "### M4 (Exp/Edu Comparator) - Resume Feature Extraction\n",
    "\n",
    "**What M4 needs:** (resume_features, jd_features) pairs with match scores.\n",
    "\n",
    "**How yashpwr helps:** The assistant summary + structured resume text provides:\n",
    "- Years of experience, education level/field, job titles, domain\n",
    "- Can synthesize matching/mismatching JD pairs with varying quality scores\n",
    "\n",
    "```python\n",
    "# Pseudocode: generate M4 pairs from yashpwr\n",
    "for resume in yashpwr_resumes:\n",
    "    features = extract_features(resume)  # years, edu, titles, skills\n",
    "    jd_match = synthesize_matching_jd(features)      # label ~0.85\n",
    "    jd_mismatch = synthesize_mismatching_jd(features) # label ~0.2\n",
    "```\n",
    "\n",
    "**Expected yield:** ~45K pairs.\n",
    "\n",
    "---\n",
    "\n",
    "### M5 (Judge) - Score Calibration\n",
    "\n",
    "**What M5 needs:** Combined M3+M4 scores mapped to overall match quality.\n",
    "\n",
    "**How yashpwr helps:** Assistant summaries provide implicit quality signals.\n",
    "Best used AFTER M3/M4 are trained (needs their outputs as input features).\n",
    "\n",
    "---\n",
    "\n",
    "### Priority Table\n",
    "\n",
    "| Model | Effort | Impact | Priority |\n",
    "|-------|--------|--------|----------|\n",
    "| **M2** (done above) | Low | High - adds ~20K sequences | Already implemented |\n",
    "| **M3** | Medium | Medium - adds skill co-occurrence | Second priority |\n",
    "| **M4** | Medium | Low - already has good data | Nice to have |\n",
    "| **M5** | High | Low - needs M3/M4 first | Future work |"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Optional: yashpwr Weak Supervision - extract NER from chat/instruction format\n# Run this BEFORE Cell 5 (Clean & split), then re-run Cells 5-9.\n\ndef load_yashpwr_weak_supervision():\n    \"\"\"Parse yashpwr chat messages into weakly-labeled NER sequences.\"\"\"\n    parquet_path = DATA_DIR / \"yashpwr_resume_ner\" / \"train.parquet\"\n    if not parquet_path.exists():\n        print(\"yashpwr parquet not found -- skipping weak supervision.\")\n        return []\n\n    df = pd.read_parquet(parquet_path)\n    if \"messages\" not in df.columns:\n        print(f\"yashpwr has no 'messages' column (cols: {df.columns.tolist()}) -- skipping.\")\n        return []\n\n    print(f\"yashpwr: {len(df)} rows with chat messages. Applying weak supervision...\")\n\n    SECTION_PATTERNS = {\n        \"skills\": re.compile(\n            r\"^(Skills|Technical Skills|Core Competencies|Areas of Expertise|\"\n            r\"Skill Highlights|Additional Skills|Computer Skills|Software Skills)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n        \"education\": re.compile(\n            r\"^(Education|Academic Background|Educational Background|Qualifications)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n        \"experience\": re.compile(\n            r\"^(Professional Experience|Work Experience|Experience|Employment|\"\n            r\"Employment History|Work History|Professional Background|Career History)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n        \"certifications\": re.compile(\n            r\"^(Certifications?|Licenses?|Professional Certifications?|\"\n            r\"Licenses? and Certifications?)\\b\",\n            re.IGNORECASE | re.MULTILINE\n        ),\n    }\n\n    EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\")\n    PHONE_RE = re.compile(r\"(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\")\n    YEARS_EXP_RE = re.compile(r\"\\b(\\d{1,2})\\+?\\s*(?:years?|yrs?)\\s*(?:of\\s+)?(?:experience)?\\b\", re.IGNORECASE)\n    YEAR_RE = re.compile(r\"\\b(19[89]\\d|20[0-2]\\d)\\b\")\n    LOCATION_RE = re.compile(r\"\\b([A-Z][a-z]+(?:\\s[A-Z][a-z]+)?)\\s*,\\s*([A-Z]{2})\\b\")\n    COMPANY_RE = re.compile(r\"Company\\s+Name\\s+(.+?)(?:\\s+City\\s*,|\\s*$)\", re.IGNORECASE | re.MULTILINE)\n    DEGREE_RE = re.compile(\n        r\"\\b(Ph\\.?D\\.?|M\\.?D\\.?|J\\.?D\\.?|M\\.?B\\.?A\\.?|M\\.?S\\.?|B\\.?S\\.?|B\\.?A\\.?|M\\.?A\\.?|\"\n        r\"Bachelor(?:'?s)?(?:\\s+of\\s+\\w+)?|Master(?:'?s)?(?:\\s+of\\s+\\w+)?|\"\n        r\"Associate(?:'?s)?(?:\\s+of\\s+\\w+)?|Doctorate|Doctor of)\\b\",\n        re.IGNORECASE\n    )\n    TITLE_RE = re.compile(\n        r\"\\b((?:Senior|Junior|Lead|Chief|Head|Principal|Staff|Associate|Assistant|Executive|\"\n        r\"Vice President|VP|Director|Manager|Coordinator|Specialist|Analyst|Engineer|\"\n        r\"Developer|Designer|Consultant|Administrator|Supervisor|Officer|Architect|\"\n        r\"Technician|Representative|Advisor|Strategist|Planner)\"\n        r\"(?:\\s+(?:of|for))?\"\n        r\"(?:\\s+\\w+){0,3})\"\n        r\"(?=\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December|\\d{4}|Company|$))\",\n        re.IGNORECASE | re.MULTILINE\n    )\n\n    records = []\n    skipped = 0\n\n    for idx, row in df.iterrows():\n        messages = row[\"messages\"]\n        if not isinstance(messages, (list, np.ndarray)):\n            skipped += 1\n            continue\n\n        resume_text = \"\"\n        for msg in messages:\n            if isinstance(msg, dict) and msg.get(\"role\") == \"user\":\n                content = msg.get(\"content\", \"\")\n                for separator in [\n                    \"following resume:\\n\\n\", \"following resume:\\n\",\n                    \"this resume:\\n\\n\", \"this resume:\\n\",\n                    \"resume:\\n\\n\", \"resume:\\n\",\n                ]:\n                    if separator in content:\n                        resume_text = content.split(separator, 1)[1]\n                        break\n                if not resume_text and len(content) > 200:\n                    resume_text = content\n                break\n\n        if len(resume_text) < 50:\n            skipped += 1\n            continue\n\n        resume_text = resume_text[:3000]\n        tokens, offsets = _tokenize_with_offsets(resume_text)\n        if len(tokens) < 10:\n            skipped += 1\n            continue\n\n        spans = []\n\n        for m in EMAIL_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"EMAIL\"})\n        for m in PHONE_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"PHONE\"})\n        for m in YEARS_EXP_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"YEARS_OF_EXPERIENCE\"})\n        for m in LOCATION_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"LOCATION\"})\n        for m in COMPANY_RE.finditer(resume_text):\n            company = m.group(1).strip()\n            if len(company) > 2:\n                spans.append({\"start\": m.start(1), \"end\": m.start(1) + len(company), \"label\": \"COMPANY\"})\n        for m in DEGREE_RE.finditer(resume_text):\n            spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"DEGREE\"})\n\n        # Section-aware tagging\n        sections = []\n        for sec_name, sec_re in SECTION_PATTERNS.items():\n            for m in sec_re.finditer(resume_text):\n                sections.append((m.start(), sec_name))\n        sections.sort(key=lambda x: x[0])\n\n        for i, (sec_start, sec_name) in enumerate(sections):\n            sec_end = sections[i + 1][0] if i + 1 < len(sections) else len(resume_text)\n            sec_text = resume_text[sec_start:sec_end]\n\n            if sec_name == \"skills\":\n                lines = sec_text.split(\"\\n\")[1:]\n                content = \" \".join(lines).strip()\n                skill_items = re.split(r\"[,;|]|\\band\\b\", content)\n                for item in skill_items:\n                    item = item.strip().strip(\".\")\n                    if 2 < len(item) < 50 and item and not item[0].isdigit():\n                        try:\n                            for m in re.finditer(re.escape(item), resume_text[sec_start:sec_end]):\n                                spans.append({\"start\": sec_start + m.start(), \"end\": sec_start + m.end(), \"label\": \"SKILLS\"})\n                                break\n                        except re.error:\n                            continue\n\n            elif sec_name == \"education\":\n                for m in YEAR_RE.finditer(sec_text):\n                    spans.append({\"start\": sec_start + m.start(), \"end\": sec_start + m.end(), \"label\": \"GRADUATION_YEAR\"})\n                for m in re.finditer(\n                    r\"((?:University|College|Institute|School|Academy|\"\n                    r\"Polytechnic|Conservatory)(?:\\s+of)?\\s+[\\w\\s]+?)(?:\\s*[-,\\n]|$)\",\n                    sec_text, re.IGNORECASE\n                ):\n                    name = m.group(1).strip()\n                    if len(name) > 5:\n                        spans.append({\"start\": sec_start + m.start(1), \"end\": sec_start + m.start(1) + len(name), \"label\": \"COLLEGE_NAME\"})\n\n            elif sec_name == \"certifications\":\n                for line in sec_text.split(\"\\n\")[1:]:\n                    line = line.strip()\n                    if len(line) > 5 and not re.match(r\"^\\d{4}\", line):\n                        try:\n                            for m in re.finditer(re.escape(line[:80]), resume_text):\n                                spans.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"CERTIFICATION\"})\n                                break\n                        except re.error:\n                            continue\n\n        for m in TITLE_RE.finditer(resume_text):\n            title = m.group(1).strip()\n            if len(title) > 3:\n                spans.append({\"start\": m.start(1), \"end\": m.start(1) + len(title), \"label\": \"DESIGNATION\"})\n\n        if not spans:\n            skipped += 1\n            continue\n\n        spans.sort(key=lambda s: (s[\"start\"], -(s[\"end\"] - s[\"start\"])))\n        filtered_spans = []\n        last_end = -1\n        for s in spans:\n            if s[\"start\"] >= last_end:\n                filtered_spans.append(s)\n                last_end = s[\"end\"]\n\n        tags = _bio_tags_from_char_spans(tokens, offsets, filtered_spans)\n\n        for i in range(0, len(tokens), 128):\n            ct = tokens[i:i + 128]\n            ctags = tags[i:i + 128]\n            if len(ct) >= 5 and any(t != \"O\" for t in ctags):\n                records.append({\"tokens\": ct, \"ner_tags\": _label_to_id(ctags), \"source\": \"yashpwr_weak\"})\n\n    entity_counts = {}\n    for rec in records:\n        for tag_id in rec[\"ner_tags\"]:\n            label = ID2LABEL.get(tag_id, \"O\")\n            if label != \"O\" and label.startswith(\"B-\"):\n                etype = label[2:]\n                entity_counts[etype] = entity_counts.get(etype, 0) + 1\n\n    print(f\"\\nyashpwr weak supervision results:\")\n    print(f\"  Processed: {len(df) - skipped} resumes (skipped {skipped})\")\n    print(f\"  Generated: {len(records)} sequences\")\n    print(f\"  Entity counts (B- tags):\")\n    for etype, count in sorted(entity_counts.items(), key=lambda x: -x[1]):\n        print(f\"    {etype}: {count}\")\n\n    return records\n\n\n# --- Uncomment below to add yashpwr weak supervision to all_records ---\n# yashpwr_weak = load_yashpwr_weak_supervision()\n# all_records.extend(yashpwr_weak)\n# print(f\"\\nTotal after yashpwr: {len(all_records)} sequences\")\n# # Then re-run Cells 5-9",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
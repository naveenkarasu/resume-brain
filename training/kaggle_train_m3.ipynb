{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# M3 Skills Comparator Training (Kaggle + P100 GPU)\n\nFine-tunes **TechWolf/JobBERT-v2** using contrastive learning\n(MultipleNegativesRankingLoss) on skill triplets.\n\n**Model:** Sentence-transformers with SentenceTransformerTrainer API\n**Input:** (anchor_skill, positive_skill, negative_skill) triplets\n**Output:** Skill embeddings where similar skills are close together\n\n## Setup\n1. Upload `m3_training_data.zip` as a Kaggle Dataset (it will auto-extract)\n2. Add the dataset to this notebook via the sidebar **Add Data** button\n3. Select **GPU P100** in notebook settings (Settings > Accelerator)\n4. Enable **Internet** (Settings > Internet > On)\n5. Run all cells\n6. Download the trained model from the output or via Kaggle Dataset API\n\n## Data Sources (7 datasets)\n| Dataset | Type | Expected Data |\n|---------|------|---------------|\n| TechWolf ESCO Sentences | Skill-sentence contrastive pairs | ~138K pairs |\n| Tabiya ESCO | Skill synonyms / alt labels | ~14K skills |\n| Nesta Skills Taxonomy | Skill clusters (143 clusters) | ~10K skills |\n| StackLite | SO tag co-occurrence (may be LFS placeholder) | optional |\n| Related Job Skills | Skill relationship mapping | ~4.7M |\n| Job Skill Set | Job posts with extracted skills | ~4.7M |\n| Mind Tech Ontology | Tech skill ontology with synonyms | ~5.4M |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Suppress TF/CUDA warnings + install dependencies\n",
    "#\n",
    "# Kaggle pre-loads both TensorFlow and PyTorch, causing duplicate CUDA factory\n",
    "# registration messages (cuFFT, cuDNN, cuBLAS). These are cosmetic only.\n",
    "# Setting TF_CPP_MIN_LOG_LEVEL=3 before any imports suppresses them.\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['GRPC_VERBOSITY'] = 'ERROR'\n",
    "os.environ['ABSL_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*computation placer already registered.*')\n",
    "warnings.filterwarnings('ignore', message='.*Unable to register.*factory.*')\n",
    "\n",
    "!pip install -q sentence-transformers datasets pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Check GPU\nimport torch\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU detected! Enable GPU in Settings > Accelerator.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Locate training data\n",
    "#\n",
    "# Kaggle auto-extracts uploaded zip files. When you upload m3_training_data.zip\n",
    "# as a dataset, contents are extracted into /kaggle/input/<dataset-name>/.\n",
    "# Kaggle converts underscores to hyphens, so \"m3_training_data\" becomes\n",
    "# \"/kaggle/input/m3-training-data/\".\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Expected data subdirectories\n",
    "TARGET_FOLDERS = {\n",
    "    \"techwolf_esco_sentences\", \"tabiya_esco\", \"nesta_skills_taxonomy\",\n",
    "    \"stacklite\", \"related_job_skills\", \"job_skill_set\", \"mind_tech_ontology\"\n",
    "}\n",
    "\n",
    "print(\"Available datasets in /kaggle/input/:\")\n",
    "for d in os.listdir(\"/kaggle/input/\"):\n",
    "    print(f\"  /kaggle/input/{d}/\")\n",
    "\n",
    "# Search for the directory containing our data folders\n",
    "DATA_DIR = None\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "    if any(d in TARGET_FOLDERS for d in dirs):\n",
    "        DATA_DIR = Path(root)\n",
    "        break\n",
    "    if root.count(os.sep) - \"/kaggle/input/\".count(os.sep) > 4:\n",
    "        break\n",
    "\n",
    "if DATA_DIR:\n",
    "    print(f\"\\nData root found: {DATA_DIR}\")\n",
    "    print(\"Contents:\")\n",
    "    for item in sorted(os.listdir(DATA_DIR)):\n",
    "        full = DATA_DIR / item\n",
    "        if full.is_dir():\n",
    "            count = sum(1 for _ in full.rglob(\"*\") if _.is_file())\n",
    "            print(f\"  {item}/ ({count} files)\")\n",
    "        else:\n",
    "            print(f\"  {item} ({full.stat().st_size / 1e6:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\nERROR: Could not find expected data folders!\")\n",
    "    print(\"Full /kaggle/input/ tree:\")\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "        depth = root.replace(\"/kaggle/input/\", \"\").count(os.sep)\n",
    "        indent = \"  \" * depth\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        if depth < 4:\n",
    "            for f in files[:10]:\n",
    "                print(f\"{indent}  {f}\")\n",
    "            if len(files) > 10:\n",
    "                print(f\"{indent}  ... and {len(files) - 10} more files\")\n",
    "    print(\"\\nPlease add m3_training_data.zip as a Dataset in the notebook sidebar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load all skill data sources\n",
    "#\n",
    "# Primary sources (from original training script):\n",
    "#   1. TechWolf ESCO Sentences - skill-sentence contrastive pairs\n",
    "#   2. Tabiya ESCO - skill synonyms / alt labels\n",
    "#   3. Nesta Skills Taxonomy - skill cluster assignments\n",
    "#   4. StackLite - SO tag co-occurrence (may be Git LFS placeholder)\n",
    "#\n",
    "# Bonus sources (additional data not in original script):\n",
    "#   5. Related Job Skills - skill relationship mapping\n",
    "#   6. Job Skill Set - job posts with extracted skill lists\n",
    "#   7. Mind Tech Ontology - tech skill ontology with synonyms\n",
    "\n",
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "assert DATA_DIR is not None, \"DATA_DIR not set! Re-run Cell 3.\"\n",
    "print(f\"Using DATA_DIR: {DATA_DIR}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Source 1: TechWolf ESCO Skill Sentences\n",
    "# ============================================================\n",
    "\n",
    "def load_esco_sentences():\n",
    "    \"\"\"Load TechWolf Synthetic-ESCO-Skill-Sentences.\n",
    "    Returns list of dicts: {skill, sentence}.\"\"\"\n",
    "    parquet_path = DATA_DIR / \"techwolf_esco_sentences\" / \"train.parquet\"\n",
    "    if not parquet_path.exists():\n",
    "        logger.warning(\"TechWolf ESCO sentences not found -- skipping.\")\n",
    "        return []\n",
    "\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    # Identify columns\n",
    "    skill_col, sentence_col = None, None\n",
    "    for col in df.columns:\n",
    "        cl = col.lower()\n",
    "        if \"skill\" in cl and \"sent\" not in cl:\n",
    "            skill_col = col\n",
    "        elif \"sent\" in cl or \"text\" in cl or \"description\" in cl:\n",
    "            sentence_col = col\n",
    "\n",
    "    if skill_col is None or sentence_col is None:\n",
    "        cols = df.columns.tolist()\n",
    "        if len(cols) >= 2:\n",
    "            skill_col, sentence_col = cols[0], cols[1]\n",
    "        else:\n",
    "            logger.warning(\"TechWolf: cannot identify columns. Got: %s\", df.columns.tolist())\n",
    "            return []\n",
    "\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        skill = str(row[skill_col]).strip()\n",
    "        sentence = str(row[sentence_col]).strip()\n",
    "        if skill and sentence and len(skill) > 1:\n",
    "            records.append({\"skill\": skill, \"sentence\": sentence})\n",
    "\n",
    "    logger.info(\"TechWolf ESCO: loaded %d skill-sentence pairs.\", len(records))\n",
    "    return records\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Source 2: Tabiya ESCO Synonyms\n",
    "# ============================================================\n",
    "\n",
    "def load_tabiya_synonyms():\n",
    "    \"\"\"Load Tabiya ESCO skill synonyms / alternative labels.\n",
    "    Returns dict: canonical_skill -> [alt_labels].\"\"\"\n",
    "    csv_dir = DATA_DIR / \"tabiya_esco\" / \"tabiya-esco-v1.1.1\" / \"csv\"\n",
    "    skills_csv = csv_dir / \"skills.csv\"\n",
    "\n",
    "    if not skills_csv.exists():\n",
    "        # Try alternate paths\n",
    "        for candidate in DATA_DIR.glob(\"tabiya_esco/**/skills.csv\"):\n",
    "            skills_csv = candidate\n",
    "            break\n",
    "        if not skills_csv.exists():\n",
    "            logger.warning(\"Tabiya skills.csv not found -- skipping.\")\n",
    "            return {}\n",
    "\n",
    "    df = pd.read_csv(skills_csv)\n",
    "\n",
    "    preferred_col, alt_col = None, None\n",
    "    for col in df.columns:\n",
    "        cl = col.lower()\n",
    "        if \"preferred\" in cl and \"label\" in cl:\n",
    "            preferred_col = col\n",
    "        elif \"alt\" in cl and \"label\" in cl:\n",
    "            alt_col = col\n",
    "\n",
    "    if preferred_col is None:\n",
    "        for col in df.columns:\n",
    "            if \"label\" in col.lower() or \"name\" in col.lower():\n",
    "                preferred_col = col\n",
    "                break\n",
    "\n",
    "    if preferred_col is None:\n",
    "        logger.warning(\"Tabiya: cannot identify preferred label column. Got: %s\", df.columns.tolist())\n",
    "        return {}\n",
    "\n",
    "    synonyms = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        pref = str(row[preferred_col]).strip()\n",
    "        if not pref or pref == \"nan\":\n",
    "            continue\n",
    "        alts = []\n",
    "        if alt_col and pd.notna(row.get(alt_col)):\n",
    "            alt_str = str(row[alt_col])\n",
    "            for sep in [\"|\", \"\\n\", \";\"]:\n",
    "                if sep in alt_str:\n",
    "                    alts = [a.strip() for a in alt_str.split(sep) if a.strip()]\n",
    "                    break\n",
    "            if not alts and alt_str.strip():\n",
    "                alts = [alt_str.strip()]\n",
    "        synonyms[pref] = alts\n",
    "\n",
    "    logger.info(\"Tabiya ESCO: loaded %d skills with synonyms.\", len(synonyms))\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Source 3: Nesta Skills Taxonomy Clusters\n",
    "# ============================================================\n",
    "\n",
    "def load_nesta_clusters():\n",
    "    \"\"\"Load Nesta UK Skills Taxonomy cluster assignments.\n",
    "    Returns dict: cluster_id -> [skill_names].\"\"\"\n",
    "    nesta_base = DATA_DIR / \"nesta_skills_taxonomy\"\n",
    "    clusters = defaultdict(list)\n",
    "\n",
    "    json_files = list(nesta_base.rglob(\"*.json\"))\n",
    "    csv_files = list(nesta_base.rglob(\"*.csv\"))\n",
    "\n",
    "    if not json_files and not csv_files:\n",
    "        logger.warning(\"Nesta: no data files found under %s -- skipping.\", nesta_base)\n",
    "        return {}\n",
    "\n",
    "    logger.info(\"Loading Nesta clusters from %s\", nesta_base)\n",
    "\n",
    "    # Try JSON files first (taxonomy/cluster data)\n",
    "    for jf in json_files:\n",
    "        if \"cluster\" in jf.name.lower() or \"taxonomy\" in jf.name.lower() or \"hierarchy\" in jf.name.lower():\n",
    "            try:\n",
    "                with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    for key, val in data.items():\n",
    "                        if isinstance(val, list):\n",
    "                            cluster_id = hash(key) % 1000\n",
    "                            clusters[cluster_id] = [str(v) for v in val if v]\n",
    "                        elif isinstance(val, dict) and \"skills\" in val:\n",
    "                            cluster_id = hash(key) % 1000\n",
    "                            clusters[cluster_id] = [str(s) for s in val[\"skills\"] if s]\n",
    "                        elif isinstance(val, dict):\n",
    "                            # Try nested structure\n",
    "                            for sub_key, sub_val in val.items():\n",
    "                                if isinstance(sub_val, list):\n",
    "                                    cid = hash(f\"{key}_{sub_key}\") % 10000\n",
    "                                    clusters[cid] = [str(v) for v in sub_val if v]\n",
    "                logger.info(\"Nesta: loaded clusters from %s\", jf.name)\n",
    "                if clusters:\n",
    "                    break\n",
    "            except (json.JSONDecodeError, UnicodeDecodeError):\n",
    "                continue\n",
    "\n",
    "    # Fallback: CSV files\n",
    "    if not clusters:\n",
    "        for cf in csv_files:\n",
    "            if \"skill\" in cf.name.lower() or \"cluster\" in cf.name.lower():\n",
    "                try:\n",
    "                    df = pd.read_csv(cf)\n",
    "                    skill_col, cluster_col = None, None\n",
    "                    for col in df.columns:\n",
    "                        cl = col.lower()\n",
    "                        if \"skill\" in cl or \"name\" in cl or \"label\" in cl:\n",
    "                            skill_col = col\n",
    "                        elif \"cluster\" in cl or \"group\" in cl or \"category\" in cl:\n",
    "                            cluster_col = col\n",
    "                    if skill_col and cluster_col:\n",
    "                        for _, row in df.iterrows():\n",
    "                            skill = str(row[skill_col]).strip()\n",
    "                            cid = row[cluster_col]\n",
    "                            if skill and skill != \"nan\":\n",
    "                                clusters[int(hash(str(cid)) % 1000)].append(skill)\n",
    "                        logger.info(\"Nesta: loaded clusters from %s\", cf.name)\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    logger.info(\"Nesta: %d clusters, %d total skills.\", len(clusters), sum(len(v) for v in clusters.values()))\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Source 4: StackLite Tag Co-occurrence\n",
    "# ============================================================\n",
    "\n",
    "def load_stacklite_cooccurrence():\n",
    "    \"\"\"Load StackLite tag co-occurrence. Returns dict: tag -> set(co-occurring tags).\n",
    "    Note: .gz files may be Git LFS pointers (134 bytes). Handled gracefully.\"\"\"\n",
    "    tags_path = DATA_DIR / \"stacklite\" / \"question_tags.csv.gz\"\n",
    "    if not tags_path.exists():\n",
    "        logger.warning(\"StackLite tags not found -- skipping.\")\n",
    "        return {}\n",
    "\n",
    "    # Check if file is a Git LFS pointer (< 200 bytes)\n",
    "    if tags_path.stat().st_size < 200:\n",
    "        logger.warning(\"StackLite tags file is too small (%d bytes) -- likely a Git LFS pointer. Skipping.\",\n",
    "                       tags_path.stat().st_size)\n",
    "        return {}\n",
    "\n",
    "    logger.info(\"Loading StackLite co-occurrence from %s\", tags_path)\n",
    "    question_tags = defaultdict(list)\n",
    "    try:\n",
    "        with gzip.open(str(tags_path), \"rt\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for i, row in enumerate(reader):\n",
    "                qid = int(row.get(\"Id\", row.get(\"id\", 0)))\n",
    "                tag = row.get(\"Tag\", row.get(\"tag\", \"\")).strip()\n",
    "                if qid and tag:\n",
    "                    question_tags[qid].append(tag)\n",
    "                if i >= 2_000_000:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Failed to read StackLite tags: %s\", e)\n",
    "        return {}\n",
    "\n",
    "    cooccur = defaultdict(set)\n",
    "    for qid, tags in question_tags.items():\n",
    "        for tag in tags:\n",
    "            for other in tags:\n",
    "                if other != tag:\n",
    "                    cooccur[tag].add(other)\n",
    "\n",
    "    logger.info(\"StackLite: built co-occurrence for %d tags.\", len(cooccur))\n",
    "    return cooccur\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Source 5 (Bonus): Related Job Skills\n",
    "# ============================================================\n",
    "\n",
    "def load_related_skills():\n",
    "    \"\"\"Load related_skills.csv: skill -> 10 related skills.\n",
    "    Returns dict: skill_name -> [related_skill_names].\"\"\"\n",
    "    csv_path = DATA_DIR / \"related_job_skills\" / \"related_skills.csv\"\n",
    "    if not csv_path.exists():\n",
    "        logger.warning(\"Related skills CSV not found -- skipping.\")\n",
    "        return {}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    related = {}\n",
    "    name_col = df.columns[0]  # 'name'\n",
    "    rel_cols = [c for c in df.columns if c.startswith(\"related_\")]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        name = str(row[name_col]).strip()\n",
    "        if not name or name == \"nan\":\n",
    "            continue\n",
    "        rels = []\n",
    "        for rc in rel_cols:\n",
    "            val = row.get(rc)\n",
    "            if pd.notna(val) and str(val).strip() and str(val).strip() != \"nan\":\n",
    "                rels.append(str(val).strip())\n",
    "        if rels:\n",
    "            related[name] = rels\n",
    "\n",
    "    logger.info(\"Related Skills: loaded %d skills with %d total relations.\",\n",
    "                len(related), sum(len(v) for v in related.values()))\n",
    "    return related\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Source 6 (Bonus): Job Skill Set (skills from job posts)\n",
    "# ============================================================\n",
    "\n",
    "def load_job_skill_sets():\n",
    "    \"\"\"Load all_job_post.csv: extract skill sets from job postings.\n",
    "    Returns list of skill lists (each list = skills from one job).\"\"\"\n",
    "    csv_path = DATA_DIR / \"job_skill_set\" / \"all_job_post.csv\"\n",
    "    if not csv_path.exists():\n",
    "        logger.warning(\"Job skill set CSV not found -- skipping.\")\n",
    "        return []\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    skill_col = None\n",
    "    for col in df.columns:\n",
    "        if \"skill\" in col.lower():\n",
    "            skill_col = col\n",
    "            break\n",
    "\n",
    "    if skill_col is None:\n",
    "        logger.warning(\"Job skill set: no skill column found. Cols: %s\", df.columns.tolist())\n",
    "        return []\n",
    "\n",
    "    skill_lists = []\n",
    "    for _, row in df.iterrows():\n",
    "        raw = row[skill_col]\n",
    "        if pd.isna(raw):\n",
    "            continue\n",
    "        raw = str(raw).strip()\n",
    "        # Parse Python list string like \"['skill1', 'skill2', ...]\"\n",
    "        try:\n",
    "            skills = ast.literal_eval(raw)\n",
    "            if isinstance(skills, list) and len(skills) >= 2:\n",
    "                skills = [s.strip() for s in skills if isinstance(s, str) and s.strip()]\n",
    "                if len(skills) >= 2:\n",
    "                    skill_lists.append(skills)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Try comma-separated\n",
    "            skills = [s.strip() for s in raw.split(\",\") if s.strip()]\n",
    "            if len(skills) >= 2:\n",
    "                skill_lists.append(skills)\n",
    "\n",
    "    logger.info(\"Job Skill Sets: loaded %d job postings with skill lists.\", len(skill_lists))\n",
    "    return skill_lists\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Source 7 (Bonus): Mind Tech Ontology\n",
    "# ============================================================\n",
    "\n",
    "def load_tech_ontology():\n",
    "    \"\"\"Load mind_tech_ontology aggregated skills.\n",
    "    Returns dict: skill_name -> {synonyms: [], type: [], domains: []}.\"\"\"\n",
    "    json_path = DATA_DIR / \"mind_tech_ontology\" / \"__aggregated_skills.json\"\n",
    "    if not json_path.exists():\n",
    "        logger.warning(\"Tech ontology not found -- skipping.\")\n",
    "        return {}\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    ontology = {}\n",
    "    if isinstance(data, list):\n",
    "        for entry in data:\n",
    "            name = entry.get(\"name\", \"\").strip()\n",
    "            if not name:\n",
    "                continue\n",
    "            synonyms = entry.get(\"synonyms\", [])\n",
    "            skill_type = entry.get(\"type\", [])\n",
    "            domains = entry.get(\"associatedToApplicationDomains\", [])\n",
    "            frameworks = entry.get(\"specificToFrameworks\", [])\n",
    "            langs = entry.get(\"supportedProgrammingLanguages\", [])\n",
    "            ontology[name] = {\n",
    "                \"synonyms\": [s for s in synonyms if s and s != name],\n",
    "                \"type\": skill_type,\n",
    "                \"domains\": domains,\n",
    "                \"frameworks\": frameworks,\n",
    "                \"langs\": langs,\n",
    "            }\n",
    "\n",
    "    logger.info(\"Tech Ontology: loaded %d skills.\", len(ontology))\n",
    "    return ontology\n",
    "\n",
    "\n",
    "# --- Load all sources ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Loading all skill data sources...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "esco_pairs = load_esco_sentences()\n",
    "synonyms = load_tabiya_synonyms()\n",
    "clusters = load_nesta_clusters()\n",
    "cooccur = load_stacklite_cooccurrence()\n",
    "related_skills = load_related_skills()\n",
    "job_skill_sets = load_job_skill_sets()\n",
    "tech_ontology = load_tech_ontology()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Data loading summary:\")\n",
    "print(f\"  ESCO sentence pairs:  {len(esco_pairs):>8,}\")\n",
    "print(f\"  Tabiya synonyms:      {len(synonyms):>8,}\")\n",
    "print(f\"  Nesta clusters:       {len(clusters):>8,}\")\n",
    "print(f\"  StackLite co-occur:   {len(cooccur):>8,}\")\n",
    "print(f\"  Related skills:       {len(related_skills):>8,}\")\n",
    "print(f\"  Job skill sets:       {len(job_skill_sets):>8,}\")\n",
    "print(f\"  Tech ontology:        {len(tech_ontology):>8,}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build triplets + hard negatives\n",
    "#\n",
    "# Strategy:\n",
    "#   1. Synonym-based: anchor = preferred label, positive = alt label\n",
    "#   2. Cluster-based: skills in same Nesta cluster are positives\n",
    "#   3. ESCO sentence-based: different sentences about same skill\n",
    "#   4. Related skills: skill -> related skill from related_skills.csv\n",
    "#   5. Job co-occurrence: skills from same job posting are positives\n",
    "#   6. Tech ontology: skill -> synonym from ontology\n",
    "#   7. Hard negatives: replace 30% of random negatives with co-occurring but different skills\n",
    "\n",
    "MAX_TRIPLETS = 500_000\n",
    "SEED = 42\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "# Build global skill vocabulary for negative sampling\n",
    "all_skills = list(set(\n",
    "    list(synonyms.keys())\n",
    "    + [alt for alts in synonyms.values() for alt in alts]\n",
    "    + [skill for skills in clusters.values() for skill in skills]\n",
    "    + list(cooccur.keys())\n",
    "    + list(related_skills.keys())\n",
    "    + [s for rels in related_skills.values() for s in rels]\n",
    "    + [s for slist in job_skill_sets for s in slist]\n",
    "    + list(tech_ontology.keys())\n",
    "    + [syn for info in tech_ontology.values() for syn in info[\"synonyms\"]]\n",
    "))\n",
    "\n",
    "print(f\"Global skill vocabulary: {len(all_skills):,} unique skills\")\n",
    "\n",
    "if len(all_skills) < 10:\n",
    "    raise ValueError(f\"Insufficient skill vocabulary ({len(all_skills)}) for triplet construction.\")\n",
    "\n",
    "triplets = []\n",
    "\n",
    "def _random_neg(exclude):\n",
    "    \"\"\"Pick a random skill not in the exclude set.\"\"\"\n",
    "    for _ in range(10):\n",
    "        neg = rng.choice(all_skills)\n",
    "        if neg not in exclude:\n",
    "            return neg\n",
    "    return rng.choice(all_skills)\n",
    "\n",
    "\n",
    "# --- 1. Synonym-based triplets ---\n",
    "count_before = len(triplets)\n",
    "for pref, alts in synonyms.items():\n",
    "    for alt in alts:\n",
    "        if alt == pref:\n",
    "            continue\n",
    "        neg = _random_neg({pref, alt})\n",
    "        triplets.append({\"anchor\": pref, \"positive\": alt, \"negative\": neg})\n",
    "        if len(triplets) >= MAX_TRIPLETS:\n",
    "            break\n",
    "    if len(triplets) >= MAX_TRIPLETS:\n",
    "        break\n",
    "print(f\"  Synonym-based: +{len(triplets) - count_before:,} triplets\")\n",
    "\n",
    "\n",
    "# --- 2. Cluster-based triplets ---\n",
    "count_before = len(triplets)\n",
    "cluster_list = list(clusters.items())\n",
    "all_cluster_skills = [s for skills in clusters.values() for s in skills]\n",
    "if cluster_list and all_cluster_skills and len(triplets) < MAX_TRIPLETS:\n",
    "    for cid, skills in cluster_list:\n",
    "        if len(skills) < 2:\n",
    "            continue\n",
    "        other_cluster_skills = [s for oid, oss in cluster_list if oid != cid for s in oss]\n",
    "        if not other_cluster_skills:\n",
    "            other_cluster_skills = all_skills\n",
    "        for i in range(len(skills)):\n",
    "            for j in range(i + 1, min(i + 3, len(skills))):\n",
    "                neg = rng.choice(other_cluster_skills)\n",
    "                triplets.append({\"anchor\": skills[i], \"positive\": skills[j], \"negative\": neg})\n",
    "                if len(triplets) >= MAX_TRIPLETS:\n",
    "                    break\n",
    "            if len(triplets) >= MAX_TRIPLETS:\n",
    "                break\n",
    "        if len(triplets) >= MAX_TRIPLETS:\n",
    "            break\n",
    "print(f\"  Cluster-based: +{len(triplets) - count_before:,} triplets\")\n",
    "\n",
    "\n",
    "# --- 3. ESCO sentence-based triplets ---\n",
    "count_before = len(triplets)\n",
    "skill_to_sentences = defaultdict(list)\n",
    "for pair in esco_pairs:\n",
    "    skill_to_sentences[pair[\"skill\"]].append(pair[\"sentence\"])\n",
    "\n",
    "skill_keys = list(skill_to_sentences.keys())\n",
    "if len(skill_keys) >= 2 and len(triplets) < MAX_TRIPLETS:\n",
    "    for skill in skill_keys:\n",
    "        sentences = skill_to_sentences[skill]\n",
    "        if len(sentences) < 2:\n",
    "            continue\n",
    "        for i in range(min(len(sentences), 3)):\n",
    "            for j in range(i + 1, min(len(sentences), 4)):\n",
    "                neg_skill = rng.choice(skill_keys)\n",
    "                while neg_skill == skill:\n",
    "                    neg_skill = rng.choice(skill_keys)\n",
    "                neg_sentences = skill_to_sentences[neg_skill]\n",
    "                neg_sentence = rng.choice(neg_sentences) if neg_sentences else neg_skill\n",
    "                triplets.append({\n",
    "                    \"anchor\": sentences[i],\n",
    "                    \"positive\": sentences[j],\n",
    "                    \"negative\": neg_sentence,\n",
    "                })\n",
    "                if len(triplets) >= MAX_TRIPLETS:\n",
    "                    break\n",
    "            if len(triplets) >= MAX_TRIPLETS:\n",
    "                break\n",
    "        if len(triplets) >= MAX_TRIPLETS:\n",
    "            break\n",
    "print(f\"  ESCO sentence-based: +{len(triplets) - count_before:,} triplets\")\n",
    "\n",
    "\n",
    "# --- 4. Related skills triplets (bonus) ---\n",
    "count_before = len(triplets)\n",
    "if related_skills and len(triplets) < MAX_TRIPLETS:\n",
    "    rel_keys = list(related_skills.keys())\n",
    "    for skill_name, rels in related_skills.items():\n",
    "        for rel in rels[:5]:  # Top 5 related skills\n",
    "            neg = _random_neg({skill_name, rel})\n",
    "            triplets.append({\"anchor\": skill_name, \"positive\": rel, \"negative\": neg})\n",
    "            if len(triplets) >= MAX_TRIPLETS:\n",
    "                break\n",
    "        if len(triplets) >= MAX_TRIPLETS:\n",
    "            break\n",
    "print(f\"  Related skills: +{len(triplets) - count_before:,} triplets\")\n",
    "\n",
    "\n",
    "# --- 5. Job co-occurrence triplets (bonus) ---\n",
    "count_before = len(triplets)\n",
    "if job_skill_sets and len(triplets) < MAX_TRIPLETS:\n",
    "    for skill_list in job_skill_sets:\n",
    "        if len(skill_list) < 2:\n",
    "            continue\n",
    "        # Sample pairs from same job\n",
    "        pairs_per_job = min(3, len(skill_list) * (len(skill_list) - 1) // 2)\n",
    "        for _ in range(pairs_per_job):\n",
    "            a, p = rng.sample(skill_list, 2)\n",
    "            neg = _random_neg(set(skill_list))\n",
    "            triplets.append({\"anchor\": a, \"positive\": p, \"negative\": neg})\n",
    "            if len(triplets) >= MAX_TRIPLETS:\n",
    "                break\n",
    "        if len(triplets) >= MAX_TRIPLETS:\n",
    "            break\n",
    "print(f\"  Job co-occurrence: +{len(triplets) - count_before:,} triplets\")\n",
    "\n",
    "\n",
    "# --- 6. Tech ontology synonym triplets (bonus) ---\n",
    "count_before = len(triplets)\n",
    "if tech_ontology and len(triplets) < MAX_TRIPLETS:\n",
    "    for skill_name, info in tech_ontology.items():\n",
    "        for syn in info[\"synonyms\"]:\n",
    "            neg = _random_neg({skill_name, syn})\n",
    "            triplets.append({\"anchor\": skill_name, \"positive\": syn, \"negative\": neg})\n",
    "            if len(triplets) >= MAX_TRIPLETS:\n",
    "                break\n",
    "        if len(triplets) >= MAX_TRIPLETS:\n",
    "            break\n",
    "print(f\"  Tech ontology: +{len(triplets) - count_before:,} triplets\")\n",
    "\n",
    "\n",
    "# --- Shuffle and cap ---\n",
    "rng.shuffle(triplets)\n",
    "triplets = triplets[:MAX_TRIPLETS]\n",
    "print(f\"\\nTotal triplets (before hard negatives): {len(triplets):,}\")\n",
    "\n",
    "\n",
    "# --- 7. Hard negatives: replace 30% with co-occurring but different skills ---\n",
    "# Use both StackLite co-occurrence AND related_skills for hard negatives\n",
    "hard_neg_sources = dict(cooccur)  # tag -> set of co-occurring tags\n",
    "for skill_name, rels in related_skills.items():\n",
    "    if skill_name not in hard_neg_sources:\n",
    "        hard_neg_sources[skill_name] = set(rels)\n",
    "    else:\n",
    "        hard_neg_sources[skill_name].update(rels)\n",
    "\n",
    "if hard_neg_sources:\n",
    "    n_replace = int(len(triplets) * 0.3)\n",
    "    indices = rng.sample(range(len(triplets)), min(n_replace, len(triplets)))\n",
    "    replaced = 0\n",
    "    for idx in indices:\n",
    "        anchor = triplets[idx][\"anchor\"]\n",
    "        positive = triplets[idx][\"positive\"]\n",
    "        cooccurring = hard_neg_sources.get(anchor, set()) - {positive, anchor}\n",
    "        if not cooccurring:\n",
    "            cooccurring = hard_neg_sources.get(anchor.lower(), set()) - {positive.lower(), anchor.lower()}\n",
    "        if cooccurring:\n",
    "            triplets[idx][\"negative\"] = rng.choice(list(cooccurring))\n",
    "            replaced += 1\n",
    "    print(f\"Hard negatives: replaced {replaced:,} / {len(triplets):,} negatives\")\n",
    "else:\n",
    "    print(\"No hard negative sources available (StackLite + Related Skills empty).\")\n",
    "\n",
    "print(f\"\\nFinal triplet count: {len(triplets):,}\")\n",
    "\n",
    "# Show sample triplets\n",
    "print(f\"\\nSample triplets:\")\n",
    "for t in triplets[:5]:\n",
    "    print(f\"  anchor: {t['anchor'][:60]}\")\n",
    "    print(f\"  positive: {t['positive'][:60]}\")\n",
    "    print(f\"  negative: {t['negative'][:60]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Model setup + contrastive training\n#\n# Uses SentenceTransformerTrainer API (newer, more stable than model.fit())\n# Architecture: JobBERT-v2 base (no projection head - let the model learn directly)\n# Loss: MultipleNegativesRankingLoss (in-batch negatives)\n# Training: 6 epochs, batch_size=64, fp16\n\nimport transformers\nfrom sentence_transformers import SentenceTransformer, losses, models\nfrom sentence_transformers.evaluation import TripletEvaluator\nfrom sentence_transformers.trainer import SentenceTransformerTrainer\nfrom sentence_transformers.training_args import SentenceTransformerTrainingArguments\nfrom datasets import Dataset\n\n# --- Config ---\nBASE_MODEL = \"TechWolf/JobBERT-v2\"\nEPOCHS = 6\nBATCH_SIZE = 64\nWARMUP_RATIO = 0.1\nOUTPUT_DIR = \"/kaggle/working/m3_skills_comparator\"\n\n# --- Load base model ---\nprint(f\"Loading base model: {BASE_MODEL}\")\ntransformers.logging.set_verbosity_error()\nmodel = SentenceTransformer(BASE_MODEL)\ntransformers.logging.set_verbosity_warning()\nprint(f\"Model loaded. Embedding dim: {model.get_sentence_embedding_dimension()}\")\n\n# --- Split triplets: 90% train, 10% eval ---\nnp.random.seed(SEED)\nnp.random.shuffle(triplets)\nsplit = int(len(triplets) * 0.9)\ntrain_triplets = triplets[:split]\neval_triplets = triplets[split:]\nprint(f\"Train: {len(train_triplets):,} triplets, Eval: {len(eval_triplets):,} triplets\")\n\n# --- Convert to HuggingFace Datasets (Trainer API requires this) ---\ntrain_dataset = Dataset.from_dict({\n    \"anchor\": [t[\"anchor\"] for t in train_triplets],\n    \"positive\": [t[\"positive\"] for t in train_triplets],\n    \"negative\": [t[\"negative\"] for t in train_triplets],\n})\neval_dataset = Dataset.from_dict({\n    \"anchor\": [t[\"anchor\"] for t in eval_triplets],\n    \"positive\": [t[\"positive\"] for t in eval_triplets],\n    \"negative\": [t[\"negative\"] for t in eval_triplets],\n})\nprint(f\"Train dataset: {len(train_dataset):,}, Eval dataset: {len(eval_dataset):,}\")\n\n# --- Loss ---\ntrain_loss = losses.MultipleNegativesRankingLoss(model)\n\n# --- Evaluator ---\nevaluator = TripletEvaluator(\n    anchors=[t[\"anchor\"] for t in eval_triplets],\n    positives=[t[\"positive\"] for t in eval_triplets],\n    negatives=[t[\"negative\"] for t in eval_triplets],\n    name=\"skill_triplet_eval\",\n)\n\n# --- Training args ---\neval_steps = max(1, len(train_triplets) // BATCH_SIZE // 2)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Starting contrastive training\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Warmup ratio: {WARMUP_RATIO}\")\nprint(f\"  Eval every: {eval_steps} steps\")\nprint(f\"  FP16: True\")\nprint(f\"{'='*60}\\n\")\n\nargs = SentenceTransformerTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    warmup_ratio=WARMUP_RATIO,\n    fp16=True,\n    eval_strategy=\"steps\",\n    eval_steps=eval_steps,\n    save_strategy=\"steps\",\n    save_steps=eval_steps,\n    load_best_model_at_end=True,\n    save_total_limit=2,\n    logging_steps=50,\n    report_to=\"none\",\n)\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=train_loss,\n    evaluator=evaluator,\n)\n\ntrainer.train()\nmodel.save(OUTPUT_DIR)\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Evaluate + export projection weights\n\n# Final evaluation\nprint(\"Evaluating on held-out triplets...\")\neval_result = evaluator(model, output_path=OUTPUT_DIR)\n\n# evaluator returns a dict in newer sentence-transformers versions\nif isinstance(eval_result, dict):\n    eval_score = eval_result.get('skill_triplet_eval_cosine_accuracy',\n                 eval_result.get('cosine_accuracy', 0.0))\n    print(f\"Full eval results: {eval_result}\")\nelse:\n    eval_score = eval_result\n\nTARGET = 0.80\nprint(f\"\\nTriplet accuracy: {eval_score:.4f}\")\nif eval_score >= TARGET:\n    print(f\"Target {TARGET:.2f} ACHIEVED!\")\nelse:\n    print(f\"Target {TARGET:.2f} NOT MET (got {eval_score:.4f})\")\n\n# Export projection weights for lightweight inference (if projection head exists)\nprint(\"\\nExporting projection head weights (if any)...\")\nprojection_weights = {}\nfor name, param in model.named_parameters():\n    if \"projection\" in name:\n        projection_weights[name.split(\".\")[-1]] = param.detach().cpu().numpy()\n\nif projection_weights:\n    projection_path = os.path.join(OUTPUT_DIR, \"projection.npy\")\n    np.save(projection_path, projection_weights)\n    print(f\"Projection weights saved to {projection_path}\")\nelse:\n    print(\"No projection head found (using base model embeddings directly).\")\n\n# Quick embedding sanity check\nprint(\"\\nSanity check - similar vs dissimilar skill embeddings:\")\ntest_pairs = [\n    (\"Python programming\", \"Python development\", \"should be HIGH\"),\n    (\"machine learning\", \"deep learning\", \"should be HIGH\"),\n    (\"Python programming\", \"accounting\", \"should be LOW\"),\n    (\"data science\", \"plumbing\", \"should be LOW\"),\n]\nfrom sentence_transformers.util import cos_sim\nfor s1, s2, expected in test_pairs:\n    e1 = model.encode(s1)\n    e2 = model.encode(s2)\n    sim = cos_sim(e1, e2).item()\n    print(f\"  '{s1}' vs '{s2}': {sim:.4f} ({expected})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save model + upload as Kaggle Dataset for download\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Save full model\n",
    "model.save(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# List saved files\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for f in sorted(files):\n",
    "        fp = os.path.join(root, f)\n",
    "        size_mb = os.path.getsize(fp) / 1e6\n",
    "        total_size += size_mb\n",
    "        rel = os.path.relpath(fp, OUTPUT_DIR)\n",
    "        print(f\"  {rel}: {size_mb:.1f} MB\")\n",
    "print(f\"  Total: {total_size:.1f} MB\")\n",
    "\n",
    "# Zip for download\n",
    "zip_path = \"/kaggle/working/m3_skills_comparator_trained\"\n",
    "shutil.make_archive(zip_path, \"zip\", OUTPUT_DIR)\n",
    "zip_size = os.path.getsize(f\"{zip_path}.zip\") / 1e6\n",
    "print(f\"\\nZipped to {zip_path}.zip ({zip_size:.1f} MB)\")\n",
    "\n",
    "# Upload as Kaggle Dataset for easy download\n",
    "dataset_dir = \"/kaggle/working/m3_dataset_upload\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "shutil.copy(f\"{zip_path}.zip\", dataset_dir)\n",
    "\n",
    "metadata = {\n",
    "    \"title\": \"m3-skills-comparator-trained\",\n",
    "    \"id\": \"thinkkun/m3-skills-comparator-trained\",\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "}\n",
    "with open(os.path.join(dataset_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "try:\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=dataset_dir, dir_mode=\"zip\", quiet=False)\n",
    "    print(\"\\nDataset uploaded successfully!\")\n",
    "    print(\"Download locally with:\")\n",
    "    print(\"  kaggle datasets download thinkkun/m3-skills-comparator-trained -p ./training/models/\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nDataset upload failed: {e}\")\n",
    "    print(\"\\nAlternative methods:\")\n",
    "    print(\"  1. Output tab (right sidebar) -> click download on the zip\")\n",
    "    print(\"  2. 'Save Version' -> after commit: kaggle kernels output thinkkun/<notebook-slug> -p ./training/models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}